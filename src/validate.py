"""
Validation script to test critical bug fixes before training

Tests:
1. Agent-index swap is cached per episode, not checked every step
2. Team advantages are computed correctly
3. Shaped reward events fire at plausible frequencies
4. Environment is seeded properly
"""

import os
import sys
import numpy as np
import torch
import argparse
from collections import Counter

from .env_builder import build_overcooked_env
from .models import ActorNetwork, CentralizedCritic
from .ppo import PPO
from .reward_shaping import RewardShaper
from configs.hyperparameters import HyperParams


def test_agent_swap_caching(num_episodes=10):
    """
    Test that agent-swap flag is cached once per episode,
    not recomputed on every step
    """
    print("\n" + "="*60)
    print("TEST 1: Agent-Index Swap Caching")
    print("="*60)

    env = build_overcooked_env('cramped_room', seed=42)
    shape_weights = HyperParams.get_shaped_reward_weights(0.0)
    reward_shaper = RewardShaper(env, shape_weights, 'cramped_room')

    swap_counts = []

    for ep in range(num_episodes):
        obs = env.reset()
        state = env.state
        reward_shaper.reset(state)

        # Record initial swap flag
        initial_swap = reward_shaper.swapped

        # Run a few steps and ensure swap flag doesn't change
        swap_changes = 0
        for step in range(50):
            actions = [np.random.randint(0, 6), np.random.randint(0, 6)]
            obs, rewards, done, info = env.step(actions)
            state = env.state

            # Check if swap flag changed (it shouldn't!)
            if reward_shaper.swapped != initial_swap:
                swap_changes += 1

            if done:
                break

        swap_counts.append(swap_changes)

        if swap_changes > 0:
            print(f"  ‚ùå Episode {ep+1}: Swap flag changed {swap_changes} times (BUG!)")
        else:
            print(f"  ‚úì Episode {ep+1}: Swap flag stable ({initial_swap})")

    if sum(swap_counts) == 0:
        print("\n‚úÖ PASS: Agent-swap flag is cached correctly per episode")
        return True
    else:
        print(f"\n‚ùå FAIL: Agent-swap flag changed {sum(swap_counts)} times across episodes")
        return False


def test_team_advantages(num_episodes=10):
    """
    Test that team advantages are computed correctly
    """
    print("\n" + "="*60)
    print("TEST 2: Team Advantage Computation")
    print("="*60)

    device = torch.device('cpu')
    env = build_overcooked_env('cramped_room', seed=42)

    # Create networks
    actors = [
        ActorNetwork(HyperParams.obs_dim, HyperParams.action_dim,
                    HyperParams.hidden_size, HyperParams.num_layers).to(device),
        ActorNetwork(HyperParams.obs_dim, HyperParams.action_dim,
                    HyperParams.hidden_size, HyperParams.num_layers).to(device)
    ]

    critic = CentralizedCritic(HyperParams.joint_obs_dim,
                              HyperParams.hidden_size, HyperParams.num_layers).to(device)

    ppo = PPO(actors, critic, HyperParams, device=device)

    # Collect some data
    for ep in range(num_episodes):
        obs = env.reset()
        done = False
        steps = 0

        while not done and steps < 50:
            # Add agent ID flags to match training code
            o0 = np.concatenate([obs['both_agent_obs'][0], np.array([1.0, 0.0], dtype=np.float32)])
            o1 = np.concatenate([obs['both_agent_obs'][1], np.array([0.0, 1.0], dtype=np.float32)])
            observations = [o0, o1]
            actions, log_probs, entropies, value, joint_obs = ppo.select_actions(observations)

            next_obs, rewards, done, info = env.step(actions)

            # Store in buffer
            ppo.buffer.add(observations, joint_obs, actions, log_probs, rewards, value, done)

            obs = next_obs
            steps += 1

    # Try to update (this will compute team advantages)
    try:
        # Add agent ID flags to match training code
        n0 = np.concatenate([obs['both_agent_obs'][0], np.array([1.0, 0.0], dtype=np.float32)])
        n1 = np.concatenate([obs['both_agent_obs'][1], np.array([0.0, 1.0], dtype=np.float32)])
        next_observations = [n0, n1]
        update_stats = ppo.update(next_observations)

        print(f"\n  Actor loss: {update_stats['actor_loss']:.4f}")
        print(f"  Critic loss: {update_stats['critic_loss']:.4f}")
        print(f"  Entropy: {update_stats['entropy']:.4f}")

        # Check that losses are reasonable (not NaN, not too large)
        if np.isnan(update_stats['actor_loss']) or np.isnan(update_stats['critic_loss']):
            print("\n‚ùå FAIL: Losses are NaN")
            return False

        if update_stats['actor_loss'] > 1000 or update_stats['critic_loss'] > 1000:
            print("\n‚ùå FAIL: Losses are too large")
            return False

        print("\n‚úÖ PASS: Team advantages computed correctly")
        return True

    except Exception as e:
        print(f"\n‚ùå FAIL: Error computing advantages: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_shaped_rewards(num_episodes=50):
    """
    Test that shaped reward events fire at plausible frequencies
    """
    print("\n" + "="*60)
    print("TEST 3: Shaped Reward Event Frequencies")
    print("="*60)

    env = build_overcooked_env('cramped_room', seed=42)
    shape_weights = HyperParams.get_shaped_reward_weights(0.0)
    reward_shaper = RewardShaper(env, shape_weights, 'cramped_room')

    total_events = Counter()

    for ep in range(num_episodes):
        obs = env.reset()
        state = env.state
        reward_shaper.reset(state)

        done = False
        while not done:
            # Random actions
            actions = [np.random.randint(0, 6), np.random.randint(0, 6)]
            obs, sparse_rewards, done, info = env.step(actions)
            next_state = env.state

            # Compute shaped rewards
            shaped_rewards, shaping_info = reward_shaper.compute_shaped_rewards(
                next_state, sparse_rewards, done
            )

        # Accumulate event counts
        for event_name, count in reward_shaper.event_counts.items():
            total_events[event_name] += count

    print("\n  Event counts over {} episodes:".format(num_episodes))
    for event_name, count in sorted(total_events.items()):
        avg_per_ep = count / num_episodes
        print(f"    {event_name:20s}: {count:4d} total ({avg_per_ep:.2f}/ep)")

    # Basic sanity checks
    # Random agents should occasionally pick up onions, start cooking, etc.
    # But shouldn't be delivering many soups (requires coordination)

    if total_events['onion_in_pot'] == 0:
        print("\n‚ö†Ô∏è  WARNING: No onions placed in pots (might be okay for random policy)")

    print("\n‚úÖ PASS: Shaped reward events tracked successfully")
    print("   (Frequencies look reasonable for random policy)")
    return True


def test_determinism(num_trials=3):
    """
    Test that seeding produces deterministic results
    """
    print("\n" + "="*60)
    print("TEST 4: Deterministic Seeding")
    print("="*60)

    all_rewards = []

    for trial in range(num_trials):
        # Reset all seeds
        np.random.seed(42)
        torch.manual_seed(42)

        env = build_overcooked_env('cramped_room', seed=42)

        # Run one episode with fixed actions
        obs = env.reset()
        total_reward = 0
        done = False
        step = 0

        # Fixed action sequence
        action_sequence = [0, 1, 2, 3, 4, 5] * 20  # Repeat pattern

        while not done and step < len(action_sequence):
            actions = [action_sequence[step % 6], action_sequence[(step+1) % 6]]
            obs, rewards, done, info = env.step(actions)
            total_reward += sum(rewards)
            step += 1

        all_rewards.append(total_reward)
        print(f"  Trial {trial+1}: Total reward = {total_reward}")

    # Check all rewards are identical
    if len(set(all_rewards)) == 1:
        print(f"\n‚úÖ PASS: Seeding is deterministic (all trials got {all_rewards[0]})")
        return True
    else:
        print(f"\n‚ùå FAIL: Seeding is not deterministic ({all_rewards})")
        return False


def run_all_tests():
    """Run all validation tests"""
    print("\n" + "="*60)
    print("VALIDATION TEST SUITE")
    print("Testing critical bug fixes before training")
    print("="*60)

    results = {}

    # Test 1: Agent-swap caching
    results['swap_caching'] = test_agent_swap_caching(num_episodes=10)

    # Test 2: Team advantages
    results['team_advantages'] = test_team_advantages(num_episodes=10)

    # Test 3: Shaped rewards
    results['shaped_rewards'] = test_shaped_rewards(num_episodes=50)

    # Test 4: Determinism
    results['determinism'] = test_determinism(num_trials=3)

    # Summary
    print("\n" + "="*60)
    print("VALIDATION SUMMARY")
    print("="*60)

    for test_name, passed in results.items():
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"  {test_name:20s}: {status}")

    all_passed = all(results.values())

    if all_passed:
        print("\nüéâ ALL TESTS PASSED - Safe to start training!")
        print("\nNext steps:")
        print("  1. Train on cramped_room: python src/train.py --layout cramped_room --episodes 50000")
        print("  2. Train on coordination_ring: python src/train.py --layout coordination_ring --episodes 100000")
        print("  3. Train on counter_circuit: python src/train.py --layout counter_circuit_o_1order --episodes 150000")
    else:
        print("\n‚ö†Ô∏è  SOME TESTS FAILED - Fix issues before training!")
        print("\nFailing tests indicate critical bugs that will break training.")

    print("="*60)

    return all_passed


def main():
    parser = argparse.ArgumentParser(description='Validate critical bug fixes')
    parser.add_argument('--test', type=str, default='all',
                       choices=['all', 'swap', 'advantages', 'rewards', 'determinism'],
                       help='Which test to run')

    args = parser.parse_args()

    if args.test == 'all':
        success = run_all_tests()
        sys.exit(0 if success else 1)
    elif args.test == 'swap':
        success = test_agent_swap_caching()
        sys.exit(0 if success else 1)
    elif args.test == 'advantages':
        success = test_team_advantages()
        sys.exit(0 if success else 1)
    elif args.test == 'rewards':
        success = test_shaped_rewards()
        sys.exit(0 if success else 1)
    elif args.test == 'determinism':
        success = test_determinism()
        sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()
