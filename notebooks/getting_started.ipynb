{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overcooked Multi-Agent RL - Getting Started\n",
    "\n",
    "This notebook helps you get started with the project and test your installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation Check\n",
    "\n",
    "First, verify all dependencies are installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, os.path.abspath('../src'))\n",
    "\n",
    "# Check imports\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"✓ PyTorch {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"✗ PyTorch not installed. Install from https://pytorch.org\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"✓ NumPy {np.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"✗ NumPy not installed\")\n",
    "\n",
    "try:\n",
    "    import overcooked_ai_py\n",
    "    print(f\"✓ Overcooked-AI installed\")\n",
    "except ImportError:\n",
    "    print(\"✗ Overcooked-AI not installed. Run: pip install overcooked-ai==1.1.0\")\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(f\"✓ Matplotlib installed\")\n",
    "except ImportError:\n",
    "    print(\"✗ Matplotlib not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Environment\n",
    "\n",
    "Test the Overcooked environment with random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from env_builder import build_overcooked_env\n\n# Build environment using MLAM-free builder\nlayout_name = 'cramped_room'\nenv = build_overcooked_env(layout_name, horizon=400, seed=42)\n\nprint(f\"Environment created: {layout_name}\")\nprint(f\"Horizon: 400\")\n\n# Test episode with random actions\nobs = env.reset()\nprint(f\"\\nObservation dict keys: {obs.keys()}\")\nprint(f\"Observation shape per agent: {obs['both_agent_obs'][0].shape}\")\n\ntotal_reward = 0\nnum_soups = 0\ndone = False\nsteps = 0\n\nwhile not done and steps < 100:\n    # Random actions for both agents (0-5 for 6 discrete actions)\n    import numpy as np\n    actions = [np.random.randint(0, 6), np.random.randint(0, 6)]\n    obs, rewards, done, info = env.step(actions)\n    \n    total_reward += sum(rewards)\n    if sum(rewards) > 0:\n        num_soups += sum(rewards) // 20\n    \n    steps += 1\n\nprint(f\"\\nRandom episode results (first 100 steps):\")\nprint(f\"  Steps: {steps}\")\nprint(f\"  Total reward: {total_reward}\")\nprint(f\"  Soups delivered: {num_soups}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Models\n",
    "\n",
    "Verify that our PPO models can be created and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ActorNetwork, CentralizedCritic\n",
    "from configs.hyperparameters import HyperParams\n",
    "\n",
    "# Create networks\n",
    "device = torch.device('cpu')\n",
    "\n",
    "actors = [\n",
    "    ActorNetwork(\n",
    "        obs_dim=HyperParams.obs_dim,\n",
    "        action_dim=HyperParams.action_dim,\n",
    "        hidden_size=HyperParams.hidden_size,\n",
    "        num_layers=HyperParams.num_layers\n",
    "    ).to(device),\n",
    "    ActorNetwork(\n",
    "        obs_dim=HyperParams.obs_dim,\n",
    "        action_dim=HyperParams.action_dim,\n",
    "        hidden_size=HyperParams.hidden_size,\n",
    "        num_layers=HyperParams.num_layers\n",
    "    ).to(device)\n",
    "]\n",
    "\n",
    "critic = CentralizedCritic(\n",
    "    joint_obs_dim=HyperParams.joint_obs_dim,\n",
    "    hidden_size=HyperParams.hidden_size,\n",
    "    num_layers=HyperParams.num_layers\n",
    ").to(device)\n",
    "\n",
    "print(\"Networks created successfully!\")\n",
    "print(f\"\\nActor 0 parameters: {sum(p.numel() for p in actors[0].parameters()):,}\")\n",
    "print(f\"Actor 1 parameters: {sum(p.numel() for p in actors[1].parameters()):,}\")\n",
    "print(f\"Critic parameters: {sum(p.numel() for p in critic.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "obs_test = torch.randn(1, HyperParams.obs_dim)\n",
    "joint_obs_test = torch.randn(1, HyperParams.joint_obs_dim)\n",
    "\n",
    "action_probs = actors[0](obs_test)\n",
    "value = critic(joint_obs_test)\n",
    "\n",
    "print(f\"\\nTest forward pass:\")\n",
    "print(f\"  Actor output shape: {action_probs.shape}\")\n",
    "print(f\"  Critic output shape: {value.shape}\")\n",
    "print(\"\\n✓ Models working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Training Test\n",
    "\n",
    "Run a few episodes to ensure training loop works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from ppo import PPO\nfrom env_builder import build_overcooked_env\n\n# Create PPO agent\nppo = PPO(actors, critic, HyperParams, device=device)\n\n# Rebuild environment with MLAM-free builder\nenv = build_overcooked_env('cramped_room', horizon=400, seed=42)\n\nprint(\"Running 5 test episodes...\")\n\nfor episode in range(5):\n    obs = env.reset()\n    done = False\n    episode_reward = 0\n    \n    while not done:\n        # Get observations\n        observations = [obs['both_agent_obs'][0], obs['both_agent_obs'][1]]\n        \n        # Select actions\n        actions, log_probs, entropies, value = ppo.select_actions(observations)\n        \n        # Step\n        next_obs, rewards, done, info = env.step(actions)\n        \n        # Store in buffer\n        joint_obs = np.concatenate(observations)\n        ppo.buffer.add(observations, joint_obs, actions, log_probs, rewards, value, done)\n        \n        obs = next_obs\n        episode_reward += sum(rewards)\n    \n    print(f\"  Episode {episode + 1}: Reward = {episode_reward:.1f}, Buffer size = {len(ppo.buffer)}\")\n\nprint(\"\\n✓ Training loop works!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next Steps\n",
    "\n",
    "If all cells above ran successfully, you're ready to start training!\n",
    "\n",
    "### To train agents:\n",
    "\n",
    "```bash\n",
    "# Cramped room (easiest - start here)\n",
    "python src/train.py --layout cramped_room --episodes 50000\n",
    "\n",
    "# Coordination ring\n",
    "python src/train.py --layout coordination_ring --episodes 100000\n",
    "\n",
    "# Counter circuit (hardest)\n",
    "python src/train.py --layout counter_circuit_o_1order --episodes 150000\n",
    "```\n",
    "\n",
    "### To evaluate trained agents:\n",
    "\n",
    "```bash\n",
    "# Evaluate single layout\n",
    "python src/evaluate.py --layout cramped_room --num_episodes 100\n",
    "\n",
    "# Evaluate all layouts\n",
    "python src/evaluate.py --num_episodes 100\n",
    "```\n",
    "\n",
    "### To create report graphs:\n",
    "\n",
    "```bash\n",
    "python src/visualize.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}