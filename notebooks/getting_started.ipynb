{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overcooked Multi-Agent RL - Getting Started\n",
    "\n",
    "This notebook helps you get started with the project and test your installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation Check\n",
    "\n",
    "First, verify all dependencies are installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, os.path.abspath('../src'))\n",
    "\n",
    "# Check imports\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"✓ PyTorch {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"✗ PyTorch not installed. Install from https://pytorch.org\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"✓ NumPy {np.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"✗ NumPy not installed\")\n",
    "\n",
    "try:\n",
    "    import overcooked_ai_py\n",
    "    print(f\"✓ Overcooked-AI installed\")\n",
    "except ImportError:\n",
    "    print(\"✗ Overcooked-AI not installed. Run: pip install overcooked-ai==1.1.0\")\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(f\"✓ Matplotlib installed\")\n",
    "except ImportError:\n",
    "    print(\"✗ Matplotlib not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Environment\n",
    "\n",
    "Test the Overcooked environment with random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld\n",
    "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv\n",
    "\n",
    "# Build environment\n",
    "layout_name = 'cramped_room'\n",
    "mdp = OvercookedGridworld.from_layout_name(layout_name)\n",
    "env = OvercookedEnv.from_mdp(mdp, horizon=400)\n",
    "\n",
    "print(f\"Environment created: {layout_name}\")\n",
    "print(f\"Horizon: {env.horizon}\")\n",
    "\n",
    "# Test episode with random actions\n",
    "obs = env.reset()\n",
    "print(f\"\\nObservation shape per agent: {obs['both_agent_obs'][0].shape}\")\n",
    "\n",
    "total_reward = 0\n",
    "num_soups = 0\n",
    "done = False\n",
    "steps = 0\n",
    "\n",
    "while not done and steps < 100:\n",
    "    # Random actions for both agents\n",
    "    actions = [env.action_space.sample(), env.action_space.sample()]\n",
    "    obs, rewards, done, info = env.step(actions)\n",
    "    \n",
    "    total_reward += sum(rewards)\n",
    "    if sum(rewards) > 0:\n",
    "        num_soups += sum(rewards) // 20\n",
    "    \n",
    "    steps += 1\n",
    "\n",
    "print(f\"\\nRandom episode results (first 100 steps):\")\n",
    "print(f\"  Steps: {steps}\")\n",
    "print(f\"  Total reward: {total_reward}\")\n",
    "print(f\"  Soups delivered: {num_soups}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Models\n",
    "\n",
    "Verify that our PPO models can be created and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ActorNetwork, CentralizedCritic\n",
    "from configs.hyperparameters import HyperParams\n",
    "\n",
    "# Create networks\n",
    "device = torch.device('cpu')\n",
    "\n",
    "actors = [\n",
    "    ActorNetwork(\n",
    "        obs_dim=HyperParams.obs_dim,\n",
    "        action_dim=HyperParams.action_dim,\n",
    "        hidden_size=HyperParams.hidden_size,\n",
    "        num_layers=HyperParams.num_layers\n",
    "    ).to(device),\n",
    "    ActorNetwork(\n",
    "        obs_dim=HyperParams.obs_dim,\n",
    "        action_dim=HyperParams.action_dim,\n",
    "        hidden_size=HyperParams.hidden_size,\n",
    "        num_layers=HyperParams.num_layers\n",
    "    ).to(device)\n",
    "]\n",
    "\n",
    "critic = CentralizedCritic(\n",
    "    joint_obs_dim=HyperParams.joint_obs_dim,\n",
    "    hidden_size=HyperParams.hidden_size,\n",
    "    num_layers=HyperParams.num_layers\n",
    ").to(device)\n",
    "\n",
    "print(\"Networks created successfully!\")\n",
    "print(f\"\\nActor 0 parameters: {sum(p.numel() for p in actors[0].parameters()):,}\")\n",
    "print(f\"Actor 1 parameters: {sum(p.numel() for p in actors[1].parameters()):,}\")\n",
    "print(f\"Critic parameters: {sum(p.numel() for p in critic.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "obs_test = torch.randn(1, HyperParams.obs_dim)\n",
    "joint_obs_test = torch.randn(1, HyperParams.joint_obs_dim)\n",
    "\n",
    "action_probs = actors[0](obs_test)\n",
    "value = critic(joint_obs_test)\n",
    "\n",
    "print(f\"\\nTest forward pass:\")\n",
    "print(f\"  Actor output shape: {action_probs.shape}\")\n",
    "print(f\"  Critic output shape: {value.shape}\")\n",
    "print(\"\\n✓ Models working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Training Test\n",
    "\n",
    "Run a few episodes to ensure training loop works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppo import PPO\n",
    "\n",
    "# Create PPO agent\n",
    "ppo = PPO(actors, critic, HyperParams, device=device)\n",
    "\n",
    "print(\"Running 5 test episodes...\")\n",
    "\n",
    "for episode in range(5):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Get observations\n",
    "        observations = [obs['both_agent_obs'][0], obs['both_agent_obs'][1]]\n",
    "        \n",
    "        # Select actions\n",
    "        actions, log_probs, entropies, value = ppo.select_actions(observations)\n",
    "        \n",
    "        # Step\n",
    "        next_obs, rewards, done, info = env.step(actions)\n",
    "        \n",
    "        # Store in buffer\n",
    "        joint_obs = np.concatenate(observations)\n",
    "        ppo.buffer.add(observations, joint_obs, actions, log_probs, rewards, value, done)\n",
    "        \n",
    "        obs = next_obs\n",
    "        episode_reward += sum(rewards)\n",
    "    \n",
    "    print(f\"  Episode {episode + 1}: Reward = {episode_reward:.1f}, Buffer size = {len(ppo.buffer)}\")\n",
    "\n",
    "print(\"\\n✓ Training loop works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next Steps\n",
    "\n",
    "If all cells above ran successfully, you're ready to start training!\n",
    "\n",
    "### To train agents:\n",
    "\n",
    "```bash\n",
    "# Cramped room (easiest - start here)\n",
    "python src/train.py --layout cramped_room --episodes 50000\n",
    "\n",
    "# Coordination ring\n",
    "python src/train.py --layout coordination_ring --episodes 100000\n",
    "\n",
    "# Counter circuit (hardest)\n",
    "python src/train.py --layout counter_circuit_o_1order --episodes 150000\n",
    "```\n",
    "\n",
    "### To evaluate trained agents:\n",
    "\n",
    "```bash\n",
    "# Evaluate single layout\n",
    "python src/evaluate.py --layout cramped_room --num_episodes 100\n",
    "\n",
    "# Evaluate all layouts\n",
    "python src/evaluate.py --num_episodes 100\n",
    "```\n",
    "\n",
    "### To create report graphs:\n",
    "\n",
    "```bash\n",
    "python src/visualize.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
