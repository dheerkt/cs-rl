---
# PPO hyperparameters for DeepRacer training
seed:               42
cpu_only:           false
environment:        deepracer-v0
experiment_name:    time_trial_ppo
host:               127.0.0.1      # Simulator host
base_port:          8888           # Starting port for simulator containers

# Training parameters
pretrained_model:   "models/deepracer-v0__time_trial_ppo__42__1764539978_final.pt"
total_timesteps:    500000         # Vegas fine-tune (hardest track)
learning_rate:      0.00003        # Gentle updates to avoid forgetting
num_envs:           16             # Number of parallel environments (match container count)
num_steps:          128            # Steps per env (16*128=2048 total per update)
batch_size:         64             # Minibatch size for PPO updates
num_epochs:         10             # PPO update epochs per rollout

# PPO parameters
gamma:              0.99           # Discount factor
gae_lambda:         0.95           # GAE lambda parameter
clip_eps:           0.2            # PPO clipping epsilon
vf_coef:            0.5            # Value function loss coefficient
ent_coef:           0.01           # Entropy bonus coefficient
max_grad_norm:      0.5            # Gradient clipping threshold

# Logging and saving
save_interval:      50             # Save checkpoint every N updates