*.sh text eol=lf
*.launch text eol=lf
*.yaml text eol=lf
*.py text eol=lf
# Custom files and folders
/logs
*.sif

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc# set base image
FROM uzairakbar/deepracer:v0

# make required directories
RUN mkdir -p /patches
RUN mkdir -p /configs
RUN mkdir -p /logs/deepracer

# copy over patch files
COPY ./patches/* /patches

# install yq
RUN wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/bin/yq &&\
    chmod +x /usr/bin/yq

# fix broken path for sensors
RUN file1_path='/opt/amazon/markov/camera_utils.py' && \
    file2_path='/opt/amazon/install/sagemaker_rl_agent/lib/python3.6/site-packages/markov/camera_utils.py' && \
    pattern='model_name="/{}/{}"' && \
    replace='model_name="{}/{}"' && \
    sed -i -e "s|$pattern|$replace|g" "$file1_path" "$file2_path"
RUN file1_path='/opt/amazon/markov/camera_utils.py' && \
    file2_path='/opt/amazon/install/sagemaker_rl_agent/lib/python3.6/site-packages/markov/camera_utils.py' && \
    pattern='model_name="/{}".format("sub_camera")' && \
    replace='model_name="{}".format("sub_camera")' && \
    sed -i -e "s|$pattern|$replace|g" "$file1_path" "$file2_path"

# patch environment response (`info` dictionary for `env.step()`)
RUN file_path='/opt/amazon/install/sagemaker_rl_agent/lib/python3.6/site-packages/markov/multi_agent_coach/multi_agent_level_manager.py' && \
    patch='/patches/environment_response.py' && \
    cat "$patch" "$file_path" > temp && mv temp "$file_path"
RUN file_path='/opt/amazon/install/sagemaker_rl_agent/lib/python3.6/site-packages/markov/multi_agent_coach/multi_agent_level_manager.py' && \
    pattern="agent.observe(env_response)" && \
    replace="agent.observe(response(agent, self.environment, env_response))" && \
    sed -i -e "s|$pattern|$replace|g" "$file_path"

# turn off kinesis video stream
RUN file_path='/opt/amazon/src/deepracer_simulation_environment/scripts/download_params_and_roslaunch_agent.py' && \
    pattern='"publish_to_kinesis_stream:={} ".format(not yaml_file.is_leaderboard_job)' && \
    replace='"publish_to_kinesis_stream:=false "' && \
    sed -i -e "s|$pattern|$replace|g" "$file_path"
RUN file_path='/opt/amazon/install/deepracer_simulation_environment/lib/deepracer_simulation_environment/download_params_and_roslaunch_agent.py' && \
    pattern='"publish_to_kinesis_stream:={} ".format(not yaml_file.is_leaderboard_job)' && \
    replace='"publish_to_kinesis_stream:=false "' && \
    sed -i -e "s|$pattern|$replace|g" "$file_path"
RUN file_path='/opt/amazon/install/deepracer_simulation_environment/share/deepracer_simulation_environment/launch/rollout_rl_agent.launch' && \
    pattern='name="publish_to_kinesis_stream" default="true"' && \
    replace='name="publish_to_kinesis_stream" default="false"' && \
    sed -i -e "s|$pattern|$replace|g" "$file_path"
RUN file_path='/opt/amazon/install/deepracer_simulation_environment/share/deepracer_simulation_environment/launch/rollout_rl_agent.launch' && \
    pattern='name="publish_to_kinesis_stream" value="$(arg publish_to_kinesis_stream)"' && \
    replace='name="publish_to_kinesis_stream" value="false"' && \
    sed -i -e "s|$pattern|$replace|g" "$file_path"

# adjust LIDAR sensor range
RUN file1_path='/opt/amazon/markov/spawn/constants.py' && \
    file2_path='/opt/amazon/install/sagemaker_rl_agent/lib/python3.6/site-packages/markov/spawn/constants.py' && \
    pattern='LIDAR_360_DEGREE_MAX_RANGE = "0.5"' && \
    replace='LIDAR_360_DEGREE_MAX_RANGE = "1.0"' && \
    sed -i -e "s|$pattern|$replace|g" "$file1_path" "$file2_path"

RUN file1_path='/opt/amazon/markov/environments/constants.py' && \
    file2_path='/opt/amazon/install/sagemaker_rl_agent/lib/python3.6/site-packages/markov/environments/constants.py' && \
    pattern='SECTOR_LIDAR_CLIPPING_DIST = 0.5' && \
    replace='SECTOR_LIDAR_CLIPPING_DIST = 1.0' && \
    sed -i -e "s|$pattern|$replace|g" "$file1_path" "$file2_path"

RUN file_path='/opt/amazon/install/deepracer_simulation_environment/share/deepracer_simulation_environment/launch/racecar.launch' && \
    pattern='<arg name="lidar_360_degree_max_range" default="0.5" />' && \
    replace='<arg name="lidar_360_degree_max_range" default="1.0" />' && \
    sed -i -e "s|$pattern|$replace|g" "$file_path"

# patch gym agent
RUN mv -f /patches/gym_agent.py \
    /opt/amazon/install/sagemaker_rl_agent/lib/python3.6/site-packages/markov/

# use customized launch script
RUN mv -f /patches/launch-simapp-rosnodes.sh /opt/ml/code/
RUN chmod +x /opt/ml/code/launch-simapp-rosnodes.sh

# in case display is needed for kinesis
RUN mkdir -p /tmp/.X11-unix && chmod 1777 /tmp/.X11-unix

# set working directory
WORKDIR /opt/ml/code/

ENTRYPOINT ["entrypoint.sh"]
# Project 4, Spring '25 - DeepRacer

![deepracer](https://github.gatech.edu/rldm/P4_deepracer/assets/78388/86684160-fe6f-4a03-972c-078cd9a9afde)

## Clone this repository
```bash
git clone https://github.gatech.edu/rldm/P4_deepracer.git
cd P4_deepracer
```

## Setup and Install Dependencies
This project requires the following to work.
- Docker or Apptainer.
- Conda (or Python 3.10 or higher).
- Linux or Windows machine with an **Intel CPU**.

Please see the detailed setup instructions in [`SETUP.md`](https://github.gatech.edu/rldm/P4_deepracer/blob/main/SETUP.md).

## Usage

Launch the DeepRacer simulation.
```bash
source scripts/start_deepracer.sh \
    [-C=MAX_CPU; default="3"] \
    [-M=MAX_MEMORY; default="6g"]

# example:
# source scripts/start_deepracer.sh -C "3" -M "6g"
```

Interact with the environment via `gymnasium`.
```python
import gymnasium as gym
import deepracer_gym

env = gym.make('deepracer-v0')

observation, info = env.reset()

observation, reward, terminated, truncated, info = env.step(
    env.action_space.sample()
)
```
See the [packages directory](https://github.gatech.edu/rldm/P4_deepracer/tree/main/packages) and the [`usage.ipynb`](https://github.gatech.edu/rldm/P4_deepracer/tree/main/usage.ipynb) notebook for details.
# Setup

## Dependencies

### WSL Ubuntu (only for Windows)
If using Windows, please install [WSL2 and Ubuntu](https://documentation.ubuntu.com/wsl/latest/howto/install-ubuntu-wsl2/).

Also install either the [WSL extention](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-wsl) or [Remote Development extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack) for VSCode to develop in WSL.

### Docker
#### Windows
Install the [Docker Desktop for Windows](https://docs.docker.com/desktop/setup/install/windows-install/) and [configure it for WSL](https://docs.docker.com/desktop/features/wsl/).

#### Linux (Ubuntu)
Uninstall all conflicting packages.
```bash
for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done
```

Set up Docker's `apt` repository.
```bash
# Add Docker's official GPG key:
sudo apt-get update
sudo apt-get install ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

# Add the repository to Apt sources:
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "${UBUNTU_CODENAME:-$VERSION_CODENAME}") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
```

Install the Docker packages.
```bash
yes | sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
sudo docker run hello-world
```

Add to sudoer group.
```bash
sudo groupadd docker
sudo usermod -aG docker $USER
newgrp docker
```
**Restart your system**, then test the installation.
```bash
docker run hello-world
```

### Conda
#### Windows
Same as below under WSL Ubuntu.
#### Linux (Ubuntu)
```bash
mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm ~/miniconda3/miniconda.sh
source ~/miniconda3/bin/activate
conda init --all
```

## Python environment
We can use either a python `venv` or a conda environment (recommended) for this project. Instructions for both are given below.

### PyTorch
If you would like to install the GPU version of PyTorch, go to the [official PyTorch install page](https://pytorch.org/get-started/locally/) and select your system with the `pip` option. Then copy the `--index-url` value into `requirements.txt` (if none present, remove it from `requirements.txt` as well).

### Conda environment (recommended)
```bash
conda env create -f environment.yaml
conda activate deepracer
```

### Python `venv`
Python version 3.10 or above is required.
```bash
environment='.deepracer'
python -m venv "$environment"
"$environment"/bin/python -m pip install -r requirements.txt
"$environment"/bin/python -m pip install -e ./packages/
source "$environment"/bin/activate
```

## PACE ICE
We recommend students to setup the project locally. However, in cases where that may not be possible, students can explore the following PACE ICE remote compute services.

* Login to the [GeorgiaTech VPN Service](https://vpn.gatech.edu/global-protect/login.esp). Download and install the [GlobalProtecht VPN client](https://vpn.gatech.edu/global-protect/getsoftwarepage.esp).
* Using the VPN client, connect to [vpn.gatech.edu](vpn.gatech.edu) and login via your GeorgiaTech username and password.
* Connect to the PACE ICE on-demand service at [ondemand-ice.pace.gatech.edu](https://ondemand-ice.pace.gatech.edu/pun/sys/dashboard).
* Click on 'My Interactive Sessions' and select whichever one you prefer on the 'Interactive Apps' meanu (we recommend Coder or VS Code).

### Environment setup
Please note that PACE ICE machines already come with Apptainer and Conda installed (use `module load anaconda3` or `module load mamba`). As such, you can follow the instructions from the [python environment section](#Python-environment) as is. However, we recommend the following additions:

- **PyTorch:** Please use the following value for `--index-url` in the `requirements.txt` file to use CUDA with PyTorch.
```bash
--index-url https://download.pytorch.org/whl/cu126
```
- **prefix:** We recommend that you install the conda environment using a `--prefix` flag as the `~/scratch` directory to prevent using up your storage.
```bash
environment='deepracer'
scratch_directory="$HOME"/scratch/conda
conda env create -f environment.yaml \
    --prefix "$scratch_directory"/"$environment"
conda config --append envs_dirs "$scratch_directory"
conda activate "$environment"
```{
    "action_space": [
        {
            "steering_angle": 30,
            "speed": 0.6
        },
        {
            "steering_angle": 15,
            "speed": 0.6
        },
        {
            "steering_angle": 0,
            "speed": 0.6
        },
        {
            "steering_angle": -15,
            "speed": 0.6
        },
        {
            "steering_angle": -30,
            "speed": 0.6
        }
    ],
    "sensor": ["STEREO_CAMERAS", "LIDAR"],
    "neural_network": "DEEP_CONVOLUTIONAL_NETWORK_SHALLOW",
    "action_space_type": "discrete",
    "version": "4"
}WORLD_NAME:                           "reInvent2019_wide"
CHANGE_START_POSITION:                "true"
ALTERNATE_DRIVING_DIRECTION:          "true"
NUMBER_OF_OBSTACLES:                  "0"
IS_OBSTACLE_BOT_CAR:                  "false"
RANDOMIZE_OBSTACLE_LOCATIONS:         "true"
# OBJECT_POSITIONS:
#     - 0.1690708037909166, -1
#     - 0.2638102569075569, 1
#     - 0.4072827740044651, -1
#     - 0.5804718430735435, 1
#     - 0.6937442410812812, -1
#     - 0.7864867324330095, 1
IS_LANE_CHANGE:                       "false"
LOWER_LANE_CHANGE_TIME:               "3.0"
UPPER_LANE_CHANGE_TIME:               "5.0"
LANE_CHANGE_DISTANCE:                 "1.0"
NUMBER_OF_BOT_CARS:                   "0"
MIN_DISTANCE_BETWEEN_BOT_CARS:        "2.0"
RANDOMIZE_BOT_CAR_LOCATIONS:          "true"
BOT_CAR_SPEED:                        "0.2"
ENABLE_DOMAIN_RANDOMIZATION:          "false"---
# default hyper-parameters for your experiments.
# these are just dummy variables as an example.
seed:               42
cpu_only:           false
environment:        deepracer-v0
experiment_name:    time_trial
learning_rate:      2e-2
total_timesteps:    1024
gamma:              0.99def reward_function(params):
    '''
    Example of rewarding the agent to follow center line
    '''
    
    # Read input parameters
    track_width = params['track_width']
    distance_from_center = params['distance_from_center']
    
    # Calculate 3 markers that are at varying distances away from the center line
    marker_1 = 0.1 * track_width
    marker_2 = 0.25 * track_width
    marker_3 = 0.5 * track_width
    
    # Give higher reward if the car is closer to center line and vice versa
    if distance_from_center <= marker_1:
        reward = 1.0
    elif distance_from_center <= marker_2:
        reward = 0.5
    elif distance_from_center <= marker_3:
        reward = 0.1
    else:
        reward = 1e-3  # likely crashed/ close to off track
    
    return float(reward)Bootstrap: localimage
From: deepracer_base.sif

# Bootstrap: docker
# From: uzairakbar/deepracer:v0

%files
    ./patches/* /patches/

%post
    # install yq
    wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/bin/yq
    chmod +x /usr/bin/yq

    # fix broken path for sensors
    file1_path='/opt/amazon/markov/camera_utils.py'
    file2_path='/opt/amazon/install/sagemaker_rl_agent/lib/python3.6/site-packages/markov/camera_utils.py'
    pattern='model_name="/{}/{}"'
    replace='model_name="{}/{}"'
    sed -i -e "s|$pattern|$replace|g" "$file1_path" "$file2_path"
    
    file1_path='/opt/amazon/markov/camera_utils.py'
    file2_path='/opt/amazon/install/sagemaker_rl_agent/lib/python3.6/site-packages/markov/camera_utils.py'
    pattern='model_name="/{}".format("sub_camera")'
    replace='model_name="{}".format("sub_camera")'
    sed -i -e "s|$pattern|$replace|g" "$file1_path" "$file2_path"

    # patch environment response (`info` dictionary for `env.step()`)
    file_path='/opt/amazon/install/sagemaker_rl_agent/lib/python3.6/site-packages/markov/multi_agent_coach/multi_agent_level_manager.py'
    patch='/patches/environment_response.py'
    cat "$patch" "$file_path" > temp && mv temp "$file_path"
    file_path='/opt/amazon/install/sagemaker_rl_agent/lib/python3.6/site-packages/markov/multi_agent_coach/multi_agent_level_manager.py'
    pattern="agent.observe(env_response)"
    replace="agent.observe(response(agent, self.environment, env_response))"
    sed -i -e "s|$pattern|$replace|g" "$file_path"

    # turn off kinesis video stream
    file_path='/opt/amazon/src/deepracer_simulation_environment/scripts/download_params_and_roslaunch_agent.py'
    pattern='"publish_to_kinesis_stream:={} ".format(not yaml_file.is_leaderboard_job)'
    replace='"publish_to_kinesis_stream:=false "'
    sed -i -e "s|$pattern|$replace|g" "$file_path"
    file_path='/opt/amazon/install/deepracer_simulation_environment/lib/deepracer_simulation_environment/download_params_and_roslaunch_agent.py'
    pattern='"publish_to_kinesis_stream:={} ".format(not yaml_file.is_leaderboard_job)'
    replace='"publish_to_kinesis_stream:=false "'
    sed -i -e "s|$pattern|$replace|g" "$file_path"
    
    file_path='/opt/amazon/install/deepracer_simulation_environment/share/deepracer_simulation_environment/launch/rollout_rl_agent.launch'
    pattern='name="publish_to_kinesis_stream" default="true"'
    replace='name="publish_to_kinesis_stream" default="false"'
    sed -i -e "s|$pattern|$replace|g" "$file_path"
    
    file_path='/opt/amazon/install/deepracer_simulation_environment/share/deepracer_simulation_environment/launch/rollout_rl_agent.launch'
    pattern='name="publish_to_kinesis_stream" value="$(arg publish_to_kinesis_stream)"'
    replace='name="publish_to_kinesis_stream" value="false"'
    sed -i -e "s|$pattern|$replace|g" "$file_path"

    # adjust LIDAR sensor range
    file1_path='/opt/amazon/markov/spawn/constants.py'
    file2_path='/opt/amazon/install/sagemaker_rl_agent/lib/python3.6/site-packages/markov/spawn/constants.py'
    pattern='LIDAR_360_DEGREE_MAX_RANGE = "0.5"'
    replace='LIDAR_360_DEGREE_MAX_RANGE = "1.0"'
    sed -i -e "s|$pattern|$replace|g" "$file1_path" "$file2_path"

    file1_path='/opt/amazon/markov/environments/constants.py'
    file2_path='/opt/amazon/install/sagemaker_rl_agent/lib/python3.6/site-packages/markov/environments/constants.py'
    pattern='SECTOR_LIDAR_CLIPPING_DIST = 0.5'
    replace='SECTOR_LIDAR_CLIPPING_DIST = 1.0'
    sed -i -e "s|$pattern|$replace|g" "$file1_path" "$file2_path"

    file_path='/opt/amazon/install/deepracer_simulation_environment/share/deepracer_simulation_environment/launch/racecar.launch'
    pattern='<arg name="lidar_360_degree_max_range" default="0.5" />'
    replace='<arg name="lidar_360_degree_max_range" default="1.0" />'
    sed -i -e "s|$pattern|$replace|g" "$file_path"

    # patch gym agent
    mv -f /patches/gym_agent.py \
        /opt/amazon/install/sagemaker_rl_agent/lib/python3.6/site-packages/markov/

    # use customized launch script
    mv -f /patches/launch-simapp-rosnodes.sh /opt/ml/code/
    chmod +x /opt/ml/code/launch-simapp-rosnodes.sh

    # in case display is needed for kinesis
    mkdir -p /tmp/.X11-unix && chmod 1777 /tmp/.X11-unix


%environment
    export LC_ALL=C
    
%runscript
    cd /opt/ml/code/
    exec /bin/bash /opt/ml/code/entrypoint.sh "$@"
name: deepracer
channels:
    - anaconda
    - conda-forge
    - pytorch
    - defaults
dependencies:
    - conda-build
    - python=3.12
    - pip
    - pip:
        - -r requirements.txt
        - -e packages/.# A Gymnasium Wrapper for DeepRacer

## Setup
### Dependencies
- Docker or Apptainer.
- Python 3.10 or higher.
- Linux or Windows machine with Intel based CPU.

### Install
```bash
pip install -e ./
```

## Usage
### Start the simulation service
From the root of this repository, start the simulator container with the following command.
```bash
source scripts/start_deepracer.sh \
    [-C=MAX_CPU; default="3"] \
    [-M=MAX_MEMORY; default="6g"]

# example:
# source scripts/start_deepracer.sh -C "3" -M "6g"
```
You may also find other scripts under `scripts/` similarly useful to stop or restart the simulation service, etc.

To check if the container is rumming you can use the following commands.
```bash
docker ps -a            # if using Docker (local setup)
apptainer instance list # if using Apptainer (PACE ICE)
```
**Note** that the simulator is initialized by the `agent_params.json` and `environment_params.yaml` config files in the `configs/` directory. To change the simulation settings, restart it after changing these files under the `configs/` directory.

### Interact with the environment
```python
import gymnasium as gym
import deepracer_gym
from configs.reward_function import (
    # provide your own custom reward function if needed.
    # if not provided, gymnasium environment fetches it from configs.reward_function
    reward_function
)

env = gym.make(
    'deepracer-v0',
    reward_function = reward_function
)

observation, info = env.reset()

observation, reward, terminated, truncated, info = env.step(
    env.action_space.sample()
)

env.close()
```
The `terminated` flag is trigerred by the following in `info['episode_status']`.
```yaml
{
    "lap_complete": float,  # same as info['reward_params']['progress'] >= 100
    "crashed": boolean,     # same as info['reward_params']['is_crashed']
    "off_track": boolean,   # same as info['reward_params']['is_offtrack']
    "reversed": boolean,    # progress decreases for 15 consecutive steps. NOT accessible in info['reward_params'].
}
```
The `truncated` flag is trigerred by the following (not accessible in `info['reward_params']`).
```yaml
{
    "immobilized": boolean,     # move <= 0.0003 for 15 consecutive steps
    "time_up": boolean,         # 180 seconds max, or 100_000 steps max
}
```

For more details, see the [`gymnasium` API section](#gymnasium-API) below.

## Configuration
### Reward function
The `configs/reward_function.py` file defines the reward function which accepts varous [input parameters](https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-reward-function-input.html). These are also accessible in `info` varibale of `gymnasium` as `info['reward_params']`.

To get motivation for designing reward functions for different types of races, please take a look at the [AWS DeepRacer reward function examples](https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-reward-function-examples.html).

### Agent parameters
The `configs/agent_params.json` configuration file defines the agent's action and observation space. The only settings of relevance are the following[^1]:.
| Parameter | Description |
|---|---|
| `action_space_type` | Can be `discrete` or `continuous`. |
| `action_space` | Defines the action space in terms of `speed` and `steering_angle`. See examples below. |
| `sensor` | Can be `FRONT_FACING_CAMERA` (a $160\times 120$ colored image), `STEREO_CAMERAS` (two $160\times 120$ greyscale images) and/or `LIDAR` ($64$ radial readings). Two camera sensors cannot be selected at once. Only LiDAR cannot be selected. For details please refer to the [AWS DeepRacer sensors page](https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-choose-race-type.html). |

We provide two examples below:

#### Discrete actions with LiDAR + stereo camera 
```json
{
    "action_space": [
        {
            "steering_angle": 30,
            "speed": 0.6
        },
        {
            "steering_angle": 15,
            "speed": 0.6
        },
        {
            "steering_angle": 0,
            "speed": 0.6
        },
        {
            "steering_angle": -15,
            "speed": 0.6
        },
        {
            "steering_angle": -30,
            "speed": 0.6
        }
    ],
    "action_space_type": "discrete",
    "sensor": ["STEREO_CAMERAS", "LIDAR"],
    "neural_network": "DEEP_CONVOLUTIONAL_NETWORK_SHALLOW",
    "version": "4"
}
```

#### Continuous actions with LiDAR + front-facing camera
```json
{
    "action_space": {
        "steering_angle": {
            "high": 30,
            "low": -30
        },
        "speed": {
            "high": 2,
            "low": 1
        }
    },
    "action_space_type": "continuous",
    "sensor": ["FRONT_FACING_CAMERA", "LIDAR"],
    "neural_network": "DEEP_CONVOLUTIONAL_NETWORK_SHALLOW",
    "version": "4"
}
```

[^1]: Please donot change the `neural_network` and `version` variables.

### Environment parameters
The `configs/environment_params.yaml` configuration file is used to define the environment. See the [DeepRacer-for-cloud documentation](https://aws-deepracer-community.github.io/deepracer-for-cloud/reference.html) for the description of these parameters.
#### List of tracks
The following tracks can be selected by setting the `WORLD_NAME` parameter. You can find the layouts for these [here](https://github.com/aws-deepracer-community/deepracer-race-data/blob/main/raw_data/tracks/README.md).
```yaml
"Albert"
"AmericasGeneratedInclStart"
"Aragon"
"Austin"
"AWS_track"
"Belille"
"Bowtie_track"
"Canada_Race"
"Canada_Training"
"ChinaAlt_track"
"China_track"
"FS_June2020"
"hamption_open"
"hamption_pro"
"July_2020"
"jyllandsringen_open"
"jyllandsringen_pro"
"LGSWide"
"MexicoAlt_track"
"Mexico_track"
"Monaco"
"Monaco_building"
"New_YorkAlt_Track"
"New_York_Track"
"Oval_track"
"penbay_open"
"penbay_pro"
"reInvent2019_track"
"reInvent2019_wide"
"reInvent2019_wide_mirrored"
"reinvent_base"
"reinvent_base_jeremiah"
"reinvent_carpet"
"reinvent_concrete"
"reinvent_wood"
"Singapore"
"Singapore_building"
"Singapore_f1"
"Spain_track"
"Spain_track_f1"
"Straight_track"
"thunder_hill_open"
"thunder_hill_pro"
"Tokyo_Racing_track"
"Tokyo_Training_track"
"Virtual_Competition_1"
"Virtual_May19_Comp_track"
"Virtual_May19_Train_track"
"Vegas_track"
```

## `gymnasium` API
The DeepRacer environment follows the standard `gymnasium` API. Here are the key components:

### Environment Creation
```python
import gymnasium as gym
import deepracer_gym

env = gym.make('deepracer-v0')
```

### Observation Space
The observation space is a composotive space defined by a [`gymnasium.spaces.Dict`](https://gymnasium.farama.org/api/spaces/composite/) dictionary object containing the following keys and values depending on the sensors specified in `configs/agent_params.json`:
```python
{
    # two 8-bit greyscale (1 channel) images
    'STEREO_CAMERAS': Box(
        low=0, high=255, shape=(2, 120, 160)
    ),
    # one 8-bit colored (3 channel) image
    'FRONT_FACING_CAMERA': Box(
        low=0, high=255, shape=(3, 120, 160)
    ),
    'LIDAR': Box(
        low=0.15, high=float('inf'), shape=(64,)
    ),
}
```

### Action Space
Depending on the specification in `configs/agent_params.json`, the actions space can be the following. Note that for continuous action spaces, the input is normalized between -1 and 1 representing the `low` and `high` values of the respective quantity.

| Type | `gymnasium.spaces` object |
|---|---|
| Discrete | `Discrete(n)`, where `n` is 5 for [the example above](#discrete-actions-with-lidar--stereo-camera). |
| Continuous[^2] | `Box(-1, 1, shape=(n,))`, where `n` is 2 for [the example above](#continuous-actions-with-lidar--front-facing-camera). |

[^2]: For continuous action spaces, the `steering_angle` and `speed` occupy the 1st and 2nd indices of the 2D action vector/list as `[normalized_steering_angle, normalized_speed]`.

### Environment Step
```python
observation, reward, terminated, truncated, info = env.step(action)
```

The step function returns:
- `observation`: Dictionary of sensor readings
- `reward`: Float value from reward function
- `terminated`: Boolean indicating episode end due to:
  - Crash
  - Off-track
  - Reversed direction
  - Lap completion
- `truncated`: Boolean indicating episode end due to:
  - Time/Step limit
  - Immobilization
- `info`: Dictionary containing:
  - `reward_params`: Parameters used in reward calculation
  - `episode_status`: Current episode state

### Environment Reset
```python
observation, info = env.reset()
```

### Environment Close
```python
env.close()
```

### Rendering
```python
# Returns numpy array
env = gym.make('deepracer-v0', render_mode='rgb_array')
```

## Limitations, Problems and Troubleshooting
- Due to simulation limitations, the `deepracer-v0` environment **does NOT support** [environment vectorization](https://gymnasium.farama.org/api/vector/). This includes `gymnasium.vector.SyncVectorEnv`, which has to be run with a maximum of `num_envs=1`.
- If Docker does not work for you without `sudo`, please follow the instructions in [`README.md`](https://github.gatech.edu/rldm/P4_deepracer/blob/main/SETUP.md) to add it to `sudo` group.
- Please note that the first run of `scripts/start_deepracer.sh` or `scripts/restart_deepracer.sh` can be quite slow. This is because the simulator base image is downloaded (~12 GBs), built with P4 specific patches before being started. But this should be a one-time process and subsequent runs should be relatively quicker.
- We have tried to deligently test the simulator and various configurations for this project. However, it is entirely possible that some edge-cases may have gone overlooked due to limited time-constraints. Should you encounter such an edge case, please feel free to hop into an OH or reach out to a TA to get it fixed ASAP.
from gymnasium.envs.registration import register

register(
    id='deepracer-v0',
    entry_point='deepracer_gym.envs:DeepracerGymEnv'
)
from deepracer_gym.envs.deepracer_gym import DeepracerGymEnvimport os
import numpy as np
import gymnasium as gym
from loguru import logger
from gymnasium import spaces
from typing import TypeAlias, Callable
import matplotlib.pyplot as plt

from deepracer_gym.gym_adapter import DeepracerGymAdapter
from deepracer_gym.envs.utils import (
    make_action_space,
    make_observation_space,
    num_channels,
    string_to_port,
    get_host_name
)
from configs.reward_function import (
    reward_function as DEFAULT_REWARD_FUNCTION
)


ActionType: TypeAlias=(int | np.ndarray | list[float])
HOST: str='127.0.0.1'
DEFAULT_PORT: int=8888
PACE_DOMAIN: str='.pace.gatech.edu'
try:
    if get_host_name().endswith(PACE_DOMAIN):
        port = string_to_port(os.environ['USER'])
    else:
        port = DEFAULT_PORT
except:
    port = DEFAULT_PORT


class DeepracerGymEnv(gym.Env):
    metadata = {
        'render_modes': ['rgb_array', 'human'],
        'render_fps': 30
    }
    def __init__(
            self,
            host: str=HOST,
            port: int=port,
            render_mode: str='rgb_array',
            reward_function: Callable=DEFAULT_REWARD_FUNCTION,
            **kwargs
        ):
        super().__init__(**kwargs)
        logger.info(
            f'Using to port {port} for deepracer server.'
        )
        self.render_mode = render_mode
        self.action_space, self._action_metadata = make_action_space()
        self.observation_space, self._observation_metadata = make_observation_space()
        self.reward_function = reward_function
        
        if isinstance(self.action_space, spaces.Discrete):
            action_space_type='discrete'
        elif isinstance(self.action_space, spaces.Box):
            action_space_type = 'continuous'
        self.deepracer_gym_adapter = DeepracerGymAdapter(
            action_space_type, host=host, port=port
        )
    
    def reset(self, **kwargs):
        super().reset(**kwargs)
        observation, info = self.deepracer_gym_adapter.env_reset()
        return observation, info
    
    def step(self, action: ActionType):
        assert self.action_space.contains(action), \
            f'Infeasible action. Action space does not containr {action}.'
        
        observation, terminated, truncated, info = (
            self.deepracer_gym_adapter.send_action(action)
        )
        reward = self.reward_function(info['reward_params'])
        return observation, reward, terminated, truncated, info
    
    def render(self, mode='rgb_array'):
        observation, _, _, _ = self.deepracer_gym_adapter._parse_response(
            self.deepracer_gym_adapter.response
        )
        measurement = None
        for sensor in observation:
            if 'CAMERA' in sensor:
                measurement = observation[sensor]
        
        if measurement is None:
            raise ValueError(
                f'Cannot render output of sensors {list(observation.keys())}.'
            )
        
        channels = num_channels(measurement)
        if channels == 2:
            # stereo camera
            measurement = np.hstack((
                measurement[0, :, :], measurement[1, :, :]
            ))
        
        channels = num_channels(measurement)
        if channels == 1:
            # greyscale image
            measurement = np.stack(
                3 * (measurement,), axis=-1
            )
        elif channels == 3:
            # front facing camera
            measurement = measurement.transpose(1, 2, 0)
        
        if mode == 'human':
            plt.imshow(np.asarray(measurement))
            plt.axis('off')
        elif mode == 'rgb_array':
            return np.asarray(measurement)
import json
import hashlib
import platform
import numpy as np
from loguru import logger
from gymnasium import spaces


LIDAR_SHAPE: tuple[int, ...]=(64,)
CAMERA_SHAPE: tuple[int, ...]=(120, 160)                        # H x W
STEREO_CAMERA_SHAPE: tuple[int, ...]=(2,)+CAMERA_SHAPE          # C x H x W
FRONT_FACING_CAMERA_SHAPE: tuple[int, ...]=(3,)+CAMERA_SHAPE    # C x H x W
SENSOR_SPACE: dict[str, spaces.Box]={
    'LIDAR': spaces.Box(
        low=0.15, high=float('inf'), shape=LIDAR_SHAPE, dtype=np.float64
    ),
    'STEREO_CAMERAS': spaces.Box(
        low=0, high=255, shape=STEREO_CAMERA_SHAPE, dtype=np.uint8
    ),
    'FRONT_FACING_CAMERA': spaces.Box(
        low=0, high=255, shape=FRONT_FACING_CAMERA_SHAPE, dtype=np.uint8
    ),
    # TODO: Look into implementing these!
    'SECTOR_LIDAR': None,
    'LEFT_CAMERA': None
}
AGENT_PARAMS_PATH: str='configs/agent_params.json'


def validate_action_space_config(config: dict, action_space_type: str):
    # make sure action_space defined correctly
    if action_space_type == 'discrete':
        assert isinstance(config['action_space'], list), \
                f'action_space_type is discrete but action_space is not a list.'
        
        for action in config['action_space']:
            assert isinstance(action, dict), \
                f'All actions should be defined as dictionaries in action_space.'

            assert all(
                (
                    key in action
                    and
                    isinstance(action[key], (int, float))
                ) for key in ('steering_angle', 'speed')
            ), f'steering_angle or speed incorrectly defined for action in action_space.'
    elif action_space_type == 'continuous':
        assert isinstance(config['action_space'], dict), \
                f'action_space_type is continuous but action_space is not a dictionary.'
        
        assert all(
            (
                key in config['action_space']
                and
                isinstance(config['action_space'][key], dict)
            ) for key in ('steering_angle', 'speed')
        ), f'steering_angle or speed incorrectly defined for action in action_space.'
        
        for action, bounds in config['action_space'].items():
            assert all(
                (
                    key in bounds
                    and
                    isinstance(bounds[key], (int, float))
                ) for key in ('low', 'high')
            ), f'Bounds incorrectly defined for {action} in action_space.'

            assert bounds['low'] < bounds['high'], \
                f'Lower bound should be lower than upper bound for {action} in action_space.'
    else:
        raise ValueError(
            f'action_space_type can only be continuous or discrete.'
        )
    
    return True


def action_space_type(config: dict):    
    if 'action_space_type' in config:
        if config['action_space_type'] not in ('discrete', 'continuous'):
            raise ValueError(
                f'Incorrectly defined action_space_type in config file.'
            )
        space_type = config['action_space_type']
    else:
        if isinstance(config['action_space'], list):
            # assuming discrete
            space_type = 'discrete'
        elif isinstance(config['action_space'], dict):
            # assuming continuous
            space_type = 'continuous'
        else:
            raise ValueError(
                f'Incorrectly defined action_space in config file.'
            )
    return space_type


def make_action_space(config_path: str=AGENT_PARAMS_PATH):
    with open(config_path, 'r') as file:
        config = json.load(file)
    
    assert 'action_space' in config, \
        f'Action space not defined in config file {config_path}.'
    
    try:
        space_type = action_space_type(config)
    except Exception as e:
        logger.error(
            f'Incorrectly defined action_space in config file {config_path}.'
        )
        raise e
    
    assert validate_action_space_config(config, space_type)

    if space_type == 'discrete':
        size = len(config['action_space'])
        action_space = spaces.Discrete(
            size
        )
    elif space_type == 'continuous':
        action_space = spaces.Box(
            low=-1, high=1, shape=(2,), dtype=np.float64
        )
    else:
        raise ValueError(
            f'Space type can only be discrete or continuous for actions. Got {space_type} instead.'
        )
    
    return action_space, config['action_space']


def make_observation_space(config_path: str=AGENT_PARAMS_PATH):
    with open(config_path, 'r') as file:
        config = json.load(file)
    sensors: list[str]=config['sensor']
    
    for sensor in sensors:
        assert (
            (sensor in SENSOR_SPACE) 
            and 
            (SENSOR_SPACE[sensor] is not None)
        ), f'Sensor {sensor} not supported!'

    return spaces.Dict({
        sensor: SENSOR_SPACE[sensor] for sensor in sensors
    }), sensors


def num_channels(measurement: np.ndarray):
    dimensions = len(measurement.shape)
    if dimensions == 2:
        channels = 1
    elif dimensions == 3:
        channels = measurement.shape[0]
    return channels


def string_to_port(string):
    hash_bytes = hashlib.sha256(string.encode()).digest()
    hash_int = int.from_bytes(
        hash_bytes[:4], byteorder='big'     # Only first 4 bytes
    )
    port = 1024 + (hash_int % (32767 - 1024 + 1))
    return int(port)


def get_host_name():
    try:
        return platform.node()
    except:
        return 'unknown'
import zmq
import numpy as np
from typing import TypeAlias
from collections.abc import Callable

from deepracer_gym.zmq_client import DeepracerClientZMQ
from deepracer_gym.utils import (
    terminated_check, truncated_check
)


PORT: int=8888
HOST: str='127.0.0.1'
TIMEOUT_LONG: int=500_000   # ~8.3 m
TIMEOUT_SHORT: int=100_000  # ~1.7 m
DUMMY_ACTION_DISCRETE: Callable[[], int]=(
    lambda: 0
)
DUMMY_ACTION_CONTINUOUS: Callable[[], np.ndarray[float]]=(
    lambda: np.random.uniform(-1, 1, 2)
)
ActionType: TypeAlias=(int | np.ndarray | list[float])

class DeepracerGymAdapter:
    def __init__(
            self,
            action_space_type: str,
            host: str=HOST,
            port: int=PORT):
        if action_space_type == 'discrete':
            self.dummy_action = DUMMY_ACTION_DISCRETE
        elif action_space_type == 'continuous':
            self.dummy_action = DUMMY_ACTION_CONTINUOUS
        else:
            raise ValueError(
                f'Action space can only be discrete or continuous. Got {action_space_type} instead.'
            )
        self.zmq_client = DeepracerClientZMQ(host=host, port=port)
        self.zmq_client.ready()
        self.response = None
        self.done = False

    def _send_action(self, action: ActionType):
        action: dict[str, ActionType] = {'action': action}
        self.response = self.zmq_client.send_message(action)
        self.done = self.response['_game_over']
        return self.response
    
    def env_reset(self):
        if self.response is None:
            # First communication to zmq server
            self.response = self.zmq_client.recieve_response()
            # Smaller timeout after first connection
            self.zmq_client.socket.set(zmq.SNDTIMEO, TIMEOUT_SHORT)
            self.zmq_client.socket.set(zmq.RCVTIMEO, TIMEOUT_SHORT)
        elif self.done:
            pass
        else:
            while not self.done:
                self.response = self._send_action(self.dummy_action())
        
        if not isinstance(self.response['info'], dict):
            self.response['info'] = dict()
        
        # If prev_episode done and reset called, fast forward one step for new episode
        # dummy action ignored due to reset()
        step = (
            self.response['info']['reward_params']['steps']
        )
        while step != 1:
            self.response = self._send_action(self.dummy_action())
            step = (
                self.response['info']['reward_params']['steps']
            )

        observation, _, _, info = self._parse_response(self.response)
        return observation, info
    
    def send_action(self, action: ActionType):
        if self.done:
            return self._parse_response(self.response)
        response = self._send_action(action)
        return self._parse_response(response)
    
    @staticmethod
    def _parse_response(response: dict):
        info = response['info']
        if not isinstance(info, dict):
            info = dict()
        info['goal'] = response['_goal']

        game_over = response['_game_over']
        terminated = terminated_check(info['episode_status'], game_over)
        truncated = truncated_check(info['episode_status'], game_over)
        
        observation = response['_next_state']
        # channel first convention
        observation = {
            sensor: (
                measurement.transpose(-1, 0, 1) if 'CAMERA' in sensor
                else measurement
            ) for sensor, measurement in observation.items()
        }

        return observation, terminated, truncated, info
from loguru import logger


def terminated_check(episode_status: dict, game_over: bool):
    if game_over and (
        episode_status['lap_complete']
        or
        episode_status['crashed']
        or
        episode_status['reversed']
        or
        episode_status['off_track']
    ):
        return True
    return False


def truncated_check(episode_status: dict, game_over: bool):
    terminated = terminated_check(episode_status, game_over)
    # time_out or immobilized
    truncated = (game_over and not terminated)
    if truncated:
        status = [k for k, v in episode_status.items() if v]
        if not episode_status['immobilized'] and not episode_status['time_up']:
            logger.warning(
                f'Expected immibilized or time_up status for truncated episode.'
                f'Instead got {status}.'
                f'Restart deepracer to prevent unexpected behavior.'
            )
    return truncated
import zmq
import msgpack

import msgpack_numpy as m
m.patch()


PORT: int=8888
HOST: str='127.0.0.1'
TIMEOUT_LONG: int=500_000   # ~8.3 m
TIMEOUT_SHORT: int=100_000  # ~1.7 m


class DeepracerClientZMQ:
    def __init__(self, host: str=HOST, port: int=PORT):
        self.host = host
        self.port = port
        self.socket = zmq.Context().socket(zmq.REQ)

        # Large timout for first connection
        self.socket.set(zmq.SNDTIMEO, TIMEOUT_LONG)
        self.socket.set(zmq.RCVTIMEO, TIMEOUT_LONG)

        self.socket.connect(f'tcp://{self.host}:{self.port}')
    
    def ready(self):
        message: dict[str, int] = {'ready': 1}
        self._send_message(message)

    def recieve_response(self):
        packed_response = self.socket.recv()
        response = msgpack.unpackb(packed_response)
        return response

    def send_message(self, message: dict[str, object]):
        self._send_message(message)
        response = self.recieve_response()
        return response
    
    def _send_message(self, message: dict[str, object]):
        packed_message = msgpack.packb(message)
        self.socket.send(packed_message)

    def __del__(self):
        self.socket.close()#!/usr/bin/env python
import os
from setuptools import setup, find_packages


if os.path.exists('README.md'):
    # Get the long description from the README file
    try:
        with open('README.md', encoding='utf-8') as f:
            long_description = f.read()
    except:
        long_description = ''
else:
    long_description = ''


setup(
    name='deepracer_gym',
    version='0.0.1',
    author='Uzair Akbar',
    author_email='uzair.akbar@gatech.edu',
    description='A gym environment for AWS DeepRacer',
    long_description=long_description,
    long_description_content_type='text/markdown',
    url='https://github.gatech.edu/rldm/P4_deepracer',
    install_requires=[
        'numpy',
        'matplotlib',
        'gymnasium',
        'pyzmq',
        'msgpack',
        'msgpack_numpy',
        'loguru'
    ],
    packages=find_packages(),
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.10',
)
def response(agent, env, env_response):
    reward_params = {}
    episode_status = {}
    for env_agent in env.agent_list:
        agent_id = (
            'agent' if len(agent.name.split('_')) <= 1 
            else agent.name.split('_')[-1]
        )
        racecar_id = (
            'racecar' if len(env_agent.ctrl._agent_name_.split('_')) <= 1 
            else env_agent.ctrl._agent_name_.split('_')[-1]
        )
        if (
            (agent_id == racecar_id)
            or
            (agent_id == 'agent' and racecar_id == 'racecar')
        ):
            reward_params = env_agent.ctrl._reward_params_
            episode_status = env_agent.ctrl._reset_rules_manager.get_dones()
    env_response.info['reward_params'] = reward_params
    env_response.info['episode_status'] = episode_status
    return env_response
import os
import zmq
import json
import msgpack
import msgpack_numpy as m
from rl_coach.core_types import ActionInfo
from rl_coach.agents.clipped_ppo_agent import ClippedPPOAgent


m.patch()

AGENT_PARAMS_PATH = '/configs/agent_params.json'
DUMMY_ACTION_DISCRETE=0
DUMMY_ACTION_CONTINUOUS=[1.0, 1.0]
try:
    GYM_PORT=int(os.environ['GYM_PORT'])
except:
    GYM_PORT=8888


def action_space_type(config):    
    if 'action_space_type' in config:
        if config['action_space_type'] not in ('discrete', 'continuous'):
            raise ValueError(
                f'Incorrectly defined action_space_type in config file.'
            )
        space_type = config['action_space_type']
    else:
        if isinstance(config['action_space'], list):
            # assuming discrete
            space_type = 'discrete'
        elif isinstance(config['action_space'], dict):
            # assuming continuous
            space_type = 'continuous'
        else:
            raise ValueError(
                f'Incorrectly defined action_space in config file.'
            )
    return space_type


class Server:
    def __init__(self, host='0.0.0.0', port=GYM_PORT):
        self.host = host
        self.port = port
        self.socket = zmq.Context.instance().socket(zmq.REP)
        self.socket.set(zmq.SNDTIMEO, 5000)
        self.socket.bind(f'tcp://{self.host}:{self.port}')

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.socket.close()

    def run(self):
        print('Starting server...')
        while True:
            packed_msg = self.socket.recv()
            msg = msgpack.unpackb(packed_msg)

            print('Received a message')
            print(msg)

            response = {'success': True}
            packed_response = msgpack.packb(response)
            self.socket.send(packed_response)


class GymAgent(ClippedPPOAgent):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        with open(AGENT_PARAMS_PATH, 'r') as file:
            config = json.load(file)
        
        assert 'action_space' in config, \
            f'Action space not defined in config file {AGENT_PARAMS_PATH}.'
        
        action_space = action_space_type(config)
        if action_space == 'discrete':
            self.dummy_action = DUMMY_ACTION_DISCRETE
        elif action_space == 'continuous':
            self.dummy_action = DUMMY_ACTION_CONTINUOUS
        else:
            raise ValueError(
                f'Action space can only be continuous or discrete. Got {action_space} instead.'
            )

        self.server = Server()
        self._previous_done = False
        self._hard_reset = False
        self._recieved_message = None
        print(f'================= Waiting for gym client =================')

        packed_msg = self.server.socket.recv()
        msg = msgpack.unpackb(packed_msg)
        print(f'=================== Gym Client Ready! ===================')

    def observe(self, env_response):
        response_dict = env_response.__dict__
        self._previous_done = response_dict['_game_over']
        if not self._hard_reset:
            packed_response = msgpack.packb(response_dict)
            self.server.socket.send(packed_response)

            packed_msg = self.server.socket.recv()
            self._recieved_message = msgpack.unpackb(packed_msg)
        else:
            self._recieved_message = {}
            if self._previous_done:
                self._hard_reset = False
                self._recieved_message['action'] = self.dummy_action  # IGNORED DUE TO RESET
            # else:
            #     env_response.game_over = True
            #     return True
    
    def act(self):
        if not self._hard_reset:

            if (self._recieved_message.get('action') is not None):
                action = self._recieved_message['action']

            elif (self._recieved_message.get('ready') is not None):
                self._hard_reset = True
                action = self.dummy_action

        else:
            action = self.dummy_action
        return ActionInfo(action=action)
#!/bin/bash
set -e
echo 'Use this script to launch a SageMaker training job.'

RANDOM_STR="$RANDOM-$(date +%s)"

echo '----------'
echo "EVALUATION: $EVALUATION"
echo "EVAL_WORLD_NAME: $EVAL_WORLD_NAME"
echo '----------'

if [ -z "$EVALUATION" ]; then
    WORLD_NAME=$(
        cat /configs/environment_params.yaml \
        | yq .WORLD_NAME
    )
    echo "Running training mode with ${WORLD_NAME} track."
elif [ "$EVALUATION" = 'true' ]; then
    WORLD_NAME="$EVAL_WORLD_NAME"
    echo "Running evaluation mode with ${WORLD_NAME} track."
else
    WORLD_NAME=$(
        cat /configs/environment_params.yaml \
        | yq .WORLD_NAME
    )
    echo "Running training mode with ${WORLD_NAME} track."
fi

NUMBER_OF_OBSTACLES=$(
    cat /configs/environment_params.yaml \
    | yq .NUMBER_OF_OBSTACLES
)
NUMBER_OF_BOT_CARS=$(
    cat /configs/environment_params.yaml \
    | yq .NUMBER_OF_BOT_CARS
)

AWS_REGION=us-east-1
BASE_JOBNAME="rlexp-deepracer"
SM_JOBNAME="${BASE_JOBNAME}-prefix"
S3_PREFIX="sagemaker-${SM_JOBNAME}"

PRETRAINED_MODEL="" # for cloning

S3_BUCKET="aws-deepracer-bba2e912-6ef0-4c3c-a072-ce17e254bcf2"
while [[ -z "$S3_BUCKET" ]]; do
    echo -n "Enter DeepRacer S3 Bucket Name: "
    read S3_BUCKET
done

S3_ROS_LOG_BUCKET="aws-deepracer-bba2e912-6ef0-4c3c-a072-ce17e254bcf2"
while [[ -z "$S3_ROS_LOG_BUCKET" ]]; do
    echo -n "Default S3_ROS_LOG_BUCKET S3 Bucket Name to ${S3_BUCKET} "
    S3_ROS_LOG_BUCKET=S3_BUCKET
done

echo "Job name: ${SM_JOBNAME}."

# mkdir does not work with nested directories in apptainer for some reason
mkdir /${S3_BUCKET}
mkdir /${S3_BUCKET}/${S3_PREFIX}
mkdir /${S3_BUCKET}/${S3_PREFIX}/model

# Upload the reward function & model metadata
REWARD_FUNCTION_S3_KEY=${S3_PREFIX}/custom_reward_function.py
MODEL_METADATA_S3_KEY=${S3_PREFIX}/model/model_metadata.json

REWARD_FUNCTION_S3_SOURCE=/${S3_BUCKET}/${REWARD_FUNCTION_S3_KEY}
MODEL_METADATA_S3_SOURCE=/${S3_BUCKET}/${MODEL_METADATA_S3_KEY}

cp /configs/agent_params.json ${MODEL_METADATA_S3_SOURCE}
echo "Uploaded model_metadata to ${MODEL_METADATA_S3_SOURCE}"
cp /opt/ml/code/test_reward_function.py ${REWARD_FUNCTION_S3_SOURCE}
echo "Uploaded dummy reward function to ${REWARD_FUNCTION_S3_SOURCE}"

# generate local yaml file and then upload to S3 bucket for robomaker training
DEFAULT_YAML="default_training_params.yaml"
touch ${DEFAULT_YAML}
echo "WORLD_NAME:                           \"${WORLD_NAME}\"" | tee ${DEFAULT_YAML}
echo "SAGEMAKER_SHARED_S3_BUCKET:           \"${S3_BUCKET}\"" | tee -a ${DEFAULT_YAML}
echo "SAGEMAKER_SHARED_S3_PREFIX:           \"${S3_PREFIX}\"" | tee -a ${DEFAULT_YAML}
echo "TRAINING_JOB_ARN:                     \"${SAGEMAKER_TRAINING_JOB_ARN}\"" | tee -a ${DEFAULT_YAML}
echo "METRICS_S3_BUCKET:                    \"${S3_BUCKET}\"" | tee -a ${DEFAULT_YAML}
echo "METRICS_S3_OBJECT_KEY:                \"/logs/training_metrics.json\"" | tee -a ${DEFAULT_YAML}
echo "SIMTRACE_S3_BUCKET:                   \"${S3_BUCKET}\"" | tee -a ${DEFAULT_YAML}
echo "SIMTRACE_S3_PREFIX:                   \"${S3_PREFIX}/iteration-data/training\"" | tee -a ${DEFAULT_YAML}
echo "MP4_S3_BUCKET:                        \"${S3_BUCKET}\"" | tee -a ${DEFAULT_YAML}
echo "MP4_S3_OBJECT_PREFIX:                 \"${S3_PREFIX}/iteration-data/training\"" | tee -a ${DEFAULT_YAML}
echo "AWS_REGION:                           \"${AWS_REGION}\"" | tee -a ${DEFAULT_YAML}
echo "TARGET_REWARD_SCORE:                  \"None\"" | tee -a ${DEFAULT_YAML}
echo "NUMBER_OF_EPISODES:                   \"0\"" | tee -a ${DEFAULT_YAML}
echo "JOB_TYPE:                             \"TRAINING\"" | tee -a ${DEFAULT_YAML}
echo "CHANGE_START_POSITION:                \"true\"" | tee -a ${DEFAULT_YAML}
echo "ALTERNATE_DRIVING_DIRECTION:          \"true\"" | tee -a ${DEFAULT_YAML}
echo "REWARD_FILE_S3_KEY:                   \"${REWARD_FUNCTION_S3_KEY}\"" | tee -a ${DEFAULT_YAML}
echo "MODEL_METADATA_FILE_S3_KEY:           \"${MODEL_METADATA_S3_KEY}\"" | tee -a ${DEFAULT_YAML}
echo "NUMBER_OF_OBSTACLES:                  \"${NUMBER_OF_OBSTACLES}\"" | tee -a ${DEFAULT_YAML}
echo "IS_OBSTACLE_BOT_CAR:                  \"false\"" | tee -a ${DEFAULT_YAML}
echo "RANDOMIZE_OBSTACLE_LOCATIONS:         \"true\"" | tee -a ${DEFAULT_YAML}
# echo "OBJECT_POSITIONS:
#  - 0.1690708037909166, -1
#  - 0.2638102569075569, 1
#  - 0.4072827740044651, -1
#  - 0.5804718430735435, 1
#  - 0.6937442410812812, -1
#  - 0.7864867324330095, 1" | tee -a ${DEFAULT_YAML}
echo "IS_LANE_CHANGE:                       \"false\"" | tee -a ${DEFAULT_YAML}
echo "LOWER_LANE_CHANGE_TIME:               \"3.0\"" | tee -a ${DEFAULT_YAML}
echo "UPPER_LANE_CHANGE_TIME:               \"5.0\"" | tee -a ${DEFAULT_YAML}
echo "LANE_CHANGE_DISTANCE:                 \"1.0\"" | tee -a ${DEFAULT_YAML}
echo "NUMBER_OF_BOT_CARS:                   \"${NUMBER_OF_BOT_CARS}\"" | tee -a ${DEFAULT_YAML}
echo "MIN_DISTANCE_BETWEEN_BOT_CARS:        \"2.0\"" | tee -a ${DEFAULT_YAML}
echo "RANDOMIZE_BOT_CAR_LOCATIONS:          \"true\"" | tee -a ${DEFAULT_YAML}
echo "BOT_CAR_SPEED:                        \"0.2\"" | tee -a ${DEFAULT_YAML}
echo "CAR_COLOR:                            \"Blue\"" | tee -a ${DEFAULT_YAML}
echo "NUMBER_OF_RESETS:                     \"0\"" | tee -a ${DEFAULT_YAML}
echo "RACE_TYPE:                            \"HEAD_TO_BOT\"" | tee -a ${DEFAULT_YAML}
echo "ENABLE_DOMAIN_RANDOMIZATION:          \"false\"" | tee -a ${DEFAULT_YAML}
echo "DISPLAY_NAME:                         \"LongLongRacerNameBlaBlaBla\"" | tee -a ${DEFAULT_YAML}
echo "REVERSE_DIR:                          \"false\"" | tee -a ${DEFAULT_YAML}
echo "BODY_SHELL_TYPE:                      \"deepracer\"" | tee -a ${DEFAULT_YAML}
echo "IS_CONTINUOUS:                        \"false\"" | tee -a ${DEFAULT_YAML}
echo "LEADERBOARD_NAME:                     \"cs7642\"" | tee -a ${DEFAULT_YAML}

NUM_WORKERS=1
echo "NUM_WORKERS:                          \"${NUM_WORKERS}\"" | tee -a ${DEFAULT_YAML}

SOURCE_YAML="/configs/environment_params.yaml"
S3_YAML_NAME="training_params.yaml"

if [ -z "$EVALUATION" ]; then
    yq eval-all 'select(fileIndex == 0) * select(fileIndex == 1)' ${DEFAULT_YAML} ${SOURCE_YAML} | tee ${S3_YAML_NAME}
elif [ "$EVALUATION" = 'true' ]; then
    cp ${DEFAULT_YAML} ${S3_YAML_NAME}
else
    yq eval-all 'select(fileIndex == 0) * select(fileIndex == 1)' ${DEFAULT_YAML} ${SOURCE_YAML} | tee ${S3_YAML_NAME}
fi

YAML_S3_KEY=${S3_PREFIX}/${S3_YAML_NAME}
YAML_S3_SOURCE=/${S3_BUCKET}/${YAML_S3_KEY}
cp ${S3_YAML_NAME} ${YAML_S3_SOURCE}
echo "Uploaded training params to ${YAML_S3_SOURCE}"


echo "Starting sageonly.sh"

COACH_EXP_NAME=sagemaker_rl

# ros melodic
export ROS_DISTRO=melodic
export PYTHONUNBUFFERED=1
export XAUTHORITY=/root/.Xauthority

IP_ADDRESSES=$(hostname -I)
echo "HOSTNAME -I ${IP_ADDRESSES}"

# Set space as the delimiter
IFS=' '
# Read the split words into an array based on space delimiter
read -a IPS_ADDRESS_LIST <<< "$IP_ADDRESSES"
unset IFS
export ROS_IP=${IPS_ADDRESS_LIST[0]}
echo "Using ROS IP ${ROS_IP}"

unset KINESIS_VIDEO_STREAM_NAME

export APP_REGION=${AWS_REGION}
export MODEL_S3_BUCKET=${S3_BUCKET}
export MODEL_S3_PREFIX=${S3_PREFIX}
export S3_YAML_NAME=${S3_YAML_NAME}
export WORLD_NAME=${WORLD_NAME}
export SIMULATION_LAUNCH_FILE='distributed_training.launch'

export SAGEMAKER_SHARED_S3_BUCKET=${S3_BUCKET}
export SAGEMAKER_SHARED_S3_PREFIX=${S3_PREFIX}

export DEEPRACER_JOB_TYPE_ENV="SAGEONLY"

export PYTHONPATH=/opt/amazon/install/sagemaker_rl_agent/lib/python3.6/site-packages/:$PYTHONPATH

export PATH="/opt/ml/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
if which x11vnc &>/dev/null; then
    source /opt/ros/$ROS_DISTRO/setup.bash
    source /opt/amazon/install/setup.bash
    export GAZEBO_MODEL_PATH='/opt/amazon/install/deepracer_simulation_environment/share/deepracer_simulation_environment'
    
    # select random display to avoid conflicts
    # check which one is free with: ps aux | grep X
    export DISPLAY=":$(( RANDOM % 99 + 1 ))"
    Xvfb "$DISPLAY" -ac -screen 0 1400x900x24 &
    echo "Using DISPLAY=${DISPLAY}"
    
    echo "Running simulation job on single sagemaker instance..."
    echo "Check ${SIMULATION_LOG_GROUP} and ${TRAINING_LOG_GROUP} for training and simulation logs."
    # redirect stderr to stdout and have error messages sent to the same file as standard output
    roslaunch deepracer_simulation_environment $SIMULATION_LAUNCH_FILE publish_to_kinesis_stream:=false
fi--index-url https://download.pytorch.org/whl/cpu
--extra-index-url https://pypi.org/simple
numpy==2.2.4
matplotlib==3.10.1
seaborn
torch
loguru==0.7.3
standard-imghdr==3.13.0
tensorboard>=2.4.1
munch==4.0.0
gymnasium==1.1.1
stable-baselines3
moviepy==2.1.2
pyzmq==26.3.0
msgpack==1.1.0
msgpack_numpy==0.4.8
enlighten==1.14.1
scikit-image==0.25.2
jupyter==1.1.1
jupyterlab==4.3.6#!/bin/bash

# check if a command exists
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# check if on a PACE ICE machine
on_pace_ice() {
    local var="$1"
    if [[ "$var" == *pace.gatech.edu ]]; then
        return 0  # Success (matches)
    else
        return 1  # Failure (does not match)
    fi
}

export conda_env=deepracer

export base=uzairakbar/deepracer:v0
export container=deepracer
export image=deepracer

SCRATCH_DIR=''
if on_pace_ice "$HOSTNAME"; then
    SCRATCH_DIR="$HOME"/scratch
    
    # if [ -L "$HOME"/.conda ]; then
    #     echo "Conda already in scratch directory."
    # else
    #     mv "$HOME"/.conda "$SCRATCH_DIR"/.conda
    #     ln "$HOME"/.conda "$SCRATCH_DIR"/.conda
    #     echo "Moved conda to scratch directory."
    # fi
    
else
    SCRATCH_DIR="$PWD"
fi

# check for Apptainer
if command_exists apptainer; then

    rm -f "$SCRATCH_DIR"/"$image".sif
    overlay=/tmp/"$container"_overlay
    rm -rf "$overlay"

    echo "Cleaned deepracer Apptainer environment."

# check for Docker
elif command_exists docker; then
    
    docker rm "$container"

    docker image rm -f "$image"

    docker image rm -f "$base"

    docker system prune --force

    echo "Cleaned deepracer Docker environment."
fi

# check for Conda
if command_exists conda; then

    conda activate base
    conda remove --name "$conda_env" --all --yes --force

    echo "Cleaned deepracer Conda environment."
fi

# no environment found to clean
echo "Nothing more to clean!"#!/bin/bash

helpFunction()
{
    echo ""
    echo "Usage: $0 -C CPUs -M memory"
    echo -e "\t-C Maximum CPUs to allocate to the container, e.g. \"3\"."
    echo -e "\t-M Maximum memory to allocate to the container, e.g. \"6g\"."
    exit 1 # Exit script after printing help
}

while getopts "C:M:E:W:" opt
do
    case "$opt" in
        C ) cpus="$OPTARG" ;;
        M ) memory="$OPTARG" ;;
        E ) evaluation="$OPTARG" ;;
        W ) world_name="$OPTARG" ;;
        ? ) helpFunction ;; # print helpFunction in case parameter is non-existent
    esac
done;

# assign default if empty
if [ -z "$cpus" ] || [ -z "$memory" ]
then
    cpus="${cpus:=3}"
    memory="${memory:=6g}"
    echo "Capping deepracer at ${cpus} CPUs and ${memory} memory.";
fi

source scripts/stop_deepracer.sh

source scripts/start_deepracer.sh \
    -C "$cpus" \
    -M "$memory" \
    -E "$evaluation" \
    -W "$world_name"
#!/bin/bash


helpFunction()
{
    echo ""
    echo "Usage: $0 -C CPUs -M memory"
    echo -e "\t-C Maximum CPUs to allocate to the container, e.g. \"3\"."
    echo -e "\t-M Maximum memory to allocate to the container, e.g. \"6g\"."
    exit 1 # Exit script after printing help
}


# check if a command exists
command_exists() {
    command -v "$1" >/dev/null 2>&1
}


docker_image_exists() {
    local image="$1"
    if docker image inspect "$image" > /dev/null 2>&1; then
        echo "Image '$image' exists."
        return 0
    else
        echo "Image '$image' does not exist."
        return 1
    fi
}


# calculate unique(-ish) port number
string_to_port() {
  local input="$1"
  local hash
  local hash_prefix
  local hash_int
  local port_range=$((32767 - 1024 + 1))

  hash=$(echo -n "$input" | sha256sum | awk '{print $1}')
  hash_prefix=${hash:0:8}  # First 4 bytes (8 hex chars = 32 bits)
  hash_int=$((16#$hash_prefix))

  echo $((1024 + (hash_int % port_range)))
}


# check if on a PACE ICE machine
on_pace_ice() {
    local var="$1"
    if [[ "$var" == *pace.gatech.edu ]]; then
        return 0  # Success (matches)
    else
        return 1  # Failure (does not match)
    fi
}


while getopts "C:M:E:W:" opt
do
    case "$opt" in
        C ) cpus="$OPTARG" ;;
        M ) memory="$OPTARG" ;;
        E ) evaluation="$OPTARG" ;;
        W ) world_name="$OPTARG" ;;
        ? ) helpFunction ;; # print helpFunction in case parameter is non-existent
    esac
done


# assign default if empty
if [ -z "$cpus" ] || [ -z "$memory" ]
then
    cpus="${cpus:=3}"
    memory="${memory:=6g}"
    echo "Capping deepracer at ${cpus} CPUs and ${memory} memory.";
fi

patches=patches
configs=configs

mkdir -p "$configs"
mkdir -p "$patches"

export base=uzairakbar/deepracer:v0
export container=deepracer
export image=deepracer


SCRATCH_DIR=''
if on_pace_ice "$HOSTNAME"; then
    SCRATCH_DIR="$HOME"/scratch
    
    # if [ -L "$HOME"/.conda ]; then
    #     echo "Conda already in scratch directory."
    # else
    #     mv "$HOME"/.conda "$SCRATCH_DIR"/.conda
    #     ln "$HOME"/.conda "$SCRATCH_DIR"/.conda
    #     echo "Moved conda to scratch directory."
    # fi
    
else
    SCRATCH_DIR="$PWD"
fi


# check for Apptainer
if command_exists apptainer; then
    echo "Building deepracer Apptainer container."

    apptainer pull deepracer_base.sif docker://"$base"

    yes no | apptainer build --ignore-fakeroot-command "$SCRATCH_DIR"/"$image".sif deepracer.def

    GYM_PORT=$(string_to_port "$USER")
    echo "Using port $GYM_PORT for deepracer."
    
    GAZEBO_PORT=$(string_to_port "GAZEBO_$USER")        # default is 11345
    GAZEBO_MASTER_URI="http://localhost:$GAZEBO_PORT"
    echo "Using port $GAZEBO_MASTER_URI for Gazebo Master."

    ROS_PORT=$(string_to_port "ROS_$USER")              # defaults is 11311
    ROS_MASTER_URI="http://localhost:$ROS_PORT"
    echo "Using port $ROS_MASTER_URI for ROS Master."

    overlay=/tmp/"$container"_overlay
    rm -rf "$overlay" && mkdir "$overlay"
    apptainer instance run \
        --no-mount "$HOME",/tmp,/dev,/etc/hosts,/etc/localtime,/proc,/sys,/var/tmp \
        --bind configs:/configs \
        --overlay "$overlay"/:/. \
        --env EVALUATION="$evaluation",EVAL_WORLD_NAME="$world_name",GYM_PORT="$GYM_PORT",GAZEBO_MASTER_URI="$GAZEBO_MASTER_URI",ROS_MASTER_URI="$ROS_MASTER_URI" \
        "$SCRATCH_DIR"/"$image".sif "$container" \
        --cpus="$cpus" --memory="$memory"

    echo "Started deepracer Apptainer container."
    
# check for Docker
elif command_exists docker; then
    echo "Building deepracer Docker container."
    
    # pull base image
    if docker_image_exists "$base"; then
        echo "Docker image '$base' already exists. Skipping pull."
    else
        echo "Docker image '$base' not found. Pulling now..."
        docker pull "$base"
    fi

    # build P4 deepracer image
    if docker_image_exists "${image}:latest"; then
        echo "Docker image '$image' already exists. Skipping build."
    else
        echo "Docker image '$image' not found. Building now..."
        docker build -t "$image" .
        
        # prune just in case of dangling images
        docker system prune --force
    fi

    echo "Using port 8888 for deepracer."

    docker run --rm --detach \
        --name="$container" \
        -v "$PWD"/"$configs":/"$configs":ro \
        -p 8888:8888 \
        -e EVALUATION="$evaluation" \
        -e EVAL_WORLD_NAME="$world_name" \
        --cpus="$cpus" --memory="$memory" \
        "$image"
    
    echo "Started deepracer Docker container."
else
    # if neither Docker nor Apptainer is found
    echo "Neither Docker nor Apptainer is installed"
fi

sleep 2#!/bin/bash

# check if a command exists
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# kill process at specified port
kill_port() {
    local PORT=$1

    if [ -z "$PORT" ]; then
        echo "Usage: kill_port <port_number>"
        return 1
    fi

    local PID
    PID=$(lsof -ti tcp:$PORT)

    if [ -n "$PID" ]; then
        echo "Port $PORT is in use by process ID $PID. Killing it..."
        kill -9 $PID
        echo "Process $PID has been killed."
    else
        echo "Port $PORT is not in use."
    fi
}

# calculate unique(-ish) port number
string_to_port() {
  local input="$1"
  local hash
  local hash_prefix
  local hash_int
  local port_range=$((32767 - 1024 + 1))

  hash=$(echo -n "$input" | sha256sum | awk '{print $1}')
  hash_prefix=${hash:0:8}  # First 4 bytes (8 hex chars = 32 bits)
  hash_int=$((16#$hash_prefix))

  echo $((1024 + (hash_int % port_range)))
}


export container=deepracer
export image=deepracer


# check for Apptainer
if command_exists apptainer; then

    apptainer instance stop "$container" || echo "No ${container} instance running."

    overlay=/tmp/"$container"_overlay
    rm -rf "$overlay"

    my_port=$(string_to_port "$USER")

    echo "Stopped deepracer Apptainer container at port ${my_port}."

# check for Docker
elif command_exists docker; then
    
    docker stop "$container"

    my_port=8888

    echo "Stopped deepracer Docker container at port ${my_port}."

else

    # if neither Docker nor Apptainer is found
    echo "Neither Docker nor Apptainer is installed"

fi

sleep 2

# # just make sure nothing is running
# kill_port "$my_port"
echo "Killed process at port ${my_port}."import abc
import torch
import torch.nn as nn

from src.transforms import EncodeObservation


class Agent(nn.Module, abc.ABC):
    '''Boilerplate class for providing interface'''
    def __init__(self, name):
        super().__init__()
        self.name = name
    
    @abc.abstractmethod
    def get_action(self, x):
        raise NotImplementedError


class RandomAgent(Agent):
    '''
    A random agent for demonstrating usage of the environment
    '''
    def __init__(self, environment, name='random'):
        super().__init__(name=name)
        self.action_space = environment.action_space        

    def get_action(self, x):
        return self.action_space.sample()


class MyFancyAgent(Agent):
    '''
    Your own deepracer agent.
    '''
    def __init__(self, name='my_fancy_agent'):
        super().__init__(name=name)
        
        # in case you want to modify/ change/ transform
        # the observation (of course we have to un-flatten it).
        self.encoder = EncodeObservation()

        # anything else you may need...
        raise NotImplementedError
    
    def get_action(self, x):

        # un-flatten observation, and other transforms if required.
        z = self.encoder(x)

        # implement your fancy policy
        raise NotImplementedErrorimport yaml
import time
import torch
import datetime
import numpy as np
from loguru import logger
from munch import munchify
from torch.utils.tensorboard import SummaryWriter

from src.agents import RandomAgent
from src.utils import (
    device,
    set_seed,
    make_environment,
)


DEVICE = device()
HYPER_PARAMS_PATH: str='configs/hyper_params.yaml'


def tensor(x: np.array, type=torch.float, device=DEVICE) -> torch.Tensor:
    return torch.tensor(x, dtype=type, device=device)


def zeros(x: tuple, type=torch.float, device=DEVICE) -> torch.Tensor:
    return torch.zeros(x, dtype=type, device=device)


def run(hparams):
    start_time = time.time()
    
    # load hyper-params if not provided
    with open(HYPER_PARAMS_PATH, 'r') as file:
        default_hparams = yaml.safe_load(file)
    
    final_hparams = default_hparams.copy()
    final_hparams.update(hparams)
    args = munchify(final_hparams)
    
    # save parameters and/or configs if you wish
    run_name = (
        f"{args.environment}__{args.experiment_name}__{args.seed}__{int(time.time())}"
    )
    writer = SummaryWriter(f"runs/{run_name}")
    writer.add_text(
        'hyperparameters',
        "|param|value|\n|-|-|\n%s" % (
            "\n".join(
                [f"|{key}|{value}|" for key, value in vars(args).items()]
            )
        ),
    )
    
    set_seed(args.seed)

    env = make_environment(args.environment)
    agent = RandomAgent(environment=env)

    # start rolling
    observation, info = env.reset()
    for step in range(args.total_timesteps):
        
        action = agent.get_action(observation)
        observation, reward, terminated, truncated, info = env.step(
            action
        )

        # just a dummy log to give you an example
        writer.add_scalar(
            'charts/steps', step, step
        )

        if terminated or truncated:
            
            et = time.time()-start_time
            et = str(datetime.timedelta(seconds=round(et)))

            logger.info(
                f'step={step}, ' + \
                f'episodic_return={info["episode"]["r"]}, ' + \
                f'episodic_length={info["episode"]["l"]}, ' + \
                f'time_elapsed={et}'
            )
            
            writer.add_scalar(
                'charts/episodic_return', info['episode']['r'], step
            )
            writer.add_scalar(
                'charts/episodic_length', info['episode']['l'], step
            )

            break
    
    # save your agent/model often
    torch.save(
        agent, f'{agent.name}.torch'
    )
    logger.info(
        f'Model {agent.name} saved.'
    )

    env.close()
    writer.close()
import torch
import numpy as np
from math import prod
import torch.nn as nn
from deepracer_gym.envs.utils import (
    # un-flattened observation shapes
    LIDAR_SHAPE,
    STEREO_CAMERA_SHAPE,
    FRONT_FACING_CAMERA_SHAPE
)

# pre-processing parameters
LIDAR_RANGE_MAX: float=1.00
LIDAR_RANGE_MIN: float=0.15
CAMERA_MAX_MEASUREMENT: int=255

# encoder params
HIDDEN_CHANNELS: int=16
LIDAR_LATENT_DIMENSION: int=32
CAMERA_LATENT_DIMENSION: int=96

# flattened observation shapes
LIDAR_FLATTEN_SHAPE: int=prod(LIDAR_SHAPE)
STEREO_FLATTEN_CAMERA_SHAPE: int=prod(STEREO_CAMERA_SHAPE)
FRONT_FLATTEN_FACING_CAMERA_SHAPE: int=prod(FRONT_FACING_CAMERA_SHAPE)


def initialize(layer, bias_const=0.01, std=np.sqrt(2)):
    '''Example NN weight initializaiton strategy.'''
    torch.nn.init.orthogonal_(layer.weight, std)
    torch.nn.init.constant_(layer.bias, bias_const)
    return layer


def activation():
    '''Example NN activation function.'''
    return nn.LeakyReLU()


class CNN(nn.Module):
    def __init__(
            self,
            in_channels,
            out_channels,
            hidden_channels: int=HIDDEN_CHANNELS,
            one_dimensional: bool=False
        ):
        super().__init__()
        conv = nn.Conv1d if one_dimensional else nn.Conv2d
        self.network = nn.Sequential(
            initialize(conv(
                in_channels, hidden_channels, kernel_size=4, stride=2,
            )),
            activation(),
            initialize(conv(
                hidden_channels, out_channels, kernel_size=4, stride=2
            )),
            activation(),
        )

    def forward(self, x):
        return self.network(x)


class PreprocessLiDAR(nn.Module):
    '''
    boiler-plate pre-processor for camera observations
    TODO: modify this class as required in case you want to pre-process LiDAR observations.
    '''
    def forward(self, x):
        return x


class PreprocessCamera(nn.Module):
    '''
    boiler-plate pre-processor for camera observations
    TODO: modify this class as required in case you want to pre-process camera observations.
    '''
    def forward(self, x):
        return x


class PreprocessStereoCameras(PreprocessCamera):
    '''boiler-plate, in case customisation is required'''


class PreprocessFrontFacingCamera(PreprocessCamera):
    '''boiler-plate, in case customisation is required'''


class EncodeStereoCameras(nn.Module):
    '''
    example CNN encoder for Stereo Camera observations.
    '''
    def __init__(self, latent_dim=CAMERA_LATENT_DIMENSION):
        super().__init__()
        self.image_encoder = nn.Sequential(
            PreprocessStereoCameras(),
            CNN(
                in_channels=1,
                out_channels=HIDDEN_CHANNELS//2
            )
        )
        self.stereo_encoder = nn.Sequential(
            CNN(
                in_channels=HIDDEN_CHANNELS,
                out_channels=4*HIDDEN_CHANNELS # 64
            ),
        )
        self.output_layer = nn.Sequential(
            nn.Flatten(),
            initialize(
                nn.Linear(
                    10*16*HIDDEN_CHANNELS,     # 2560
                    latent_dim,
                )
            ),
            activation(),
        )

    
    def forward(self, x):
        left = x[:, 0:1, :, :]
        right = x[:, 1:2, :, :]
        left_encoded = self.image_encoder(left)
        right_encoded = self.image_encoder(right)

        stereo = torch.cat(
            (left_encoded, right_encoded), 1
        )
        stereo_encoded = self.stereo_encoder(
            stereo
        )

        return self.output_layer(
            stereo_encoded
        )


class EncodeFrontFacingCamera(nn.Module):
    '''
    example CNN encoder for Front Facing Camera observations.
    '''
    def __init__(self, latent_dim=CAMERA_LATENT_DIMENSION):
        super().__init__()
        self.image_encoder = nn.Sequential(
            PreprocessFrontFacingCamera(),
            CNN(
                in_channels=3,
                out_channels=HIDDEN_CHANNELS
            ),
            CNN(
                in_channels=HIDDEN_CHANNELS,
                out_channels=HIDDEN_CHANNELS
            )
        )
        self.output_layer = nn.Sequential(
            nn.Flatten(),
            initialize(
                nn.Linear(
                    10*16*HIDDEN_CHANNELS,     # 2560
                    latent_dim,
                )
            ),
            activation(),
        )
    
    def forward(self, x):
        image_encoded = self.image_encoder(x)

        return self.output_layer(
            image_encoded
        )


class EncodeLiDAR(nn.Module):
    '''
    example CNN encoder for LiDAR observations.
    '''
    def __init__(
            self,
            latent_dim=LIDAR_LATENT_DIMENSION,
        ):
        super().__init__()
        self.lidar_encoder = nn.Sequential(
            PreprocessLiDAR(),
            CNN(
                in_channels=1,
                out_channels=HIDDEN_CHANNELS,
                one_dimensional=True,
            ),
        )
        self.output_layer = nn.Sequential(
            nn.Flatten(),
            initialize(
                nn.Linear(
                    14*HIDDEN_CHANNELS,     # 2560
                    latent_dim,
                )
            ),
            activation(),
        )
    
    def forward(self, x):
        if x.dim() == 2:
            x = x.unsqueeze(1)  # Add channel dim if missing
        
        lidar_encoded = self.lidar_encoder(x)
        return self.output_layer(
            lidar_encoded
        )



class UnflattenObservation(nn.Module):
    def forward(self, x):
        # # trailing values are LiDAR
        # return (
        #     x[..., :-LIDAR_FLATTEN_SHAPE].view(-1, *STEREO_CAMERA_SHAPE),
        #     x[..., -LIDAR_FLATTEN_SHAPE:].view(-1, *LIDAR_SHAPE)
        # )
        # leading values are LiDAR
        return (
            x[..., LIDAR_FLATTEN_SHAPE:].view(-1, *STEREO_CAMERA_SHAPE),
            x[..., :LIDAR_FLATTEN_SHAPE].view(-1, *LIDAR_SHAPE)
        )


class EncodeObservation(nn.Module):
    '''
    use this class to aggregate all of your observation
    pre-processing and encoding functions if required.
    feel free to modify/change as required!
    '''
    def __init__(self):
        super().__init__()
        self.unflatten = UnflattenObservation()
        self.camera_encoder = EncodeStereoCameras()
        self.lidar_encoder = EncodeLiDAR()
    
    def forward(self, x):
        camera, lidar = self.unflatten(x)
        camera_encoded = self.camera_encoder(camera)
        lidar_encoded = self.lidar_encoder(lidar)

        return torch.cat(
            (camera_encoded, lidar_encoded), -1
        )
import os
import json
import yaml
import torch
import random
import shutil
import enlighten
import subprocess
import numpy as np
import pandas as pd
import seaborn as sns
import gymnasium as gym
from loguru import logger
from gymnasium import spaces
import matplotlib.pyplot as plt
from gymnasium.wrappers import (
    RecordVideo,
    FlattenObservation,
    RecordEpisodeStatistics
)
from IPython.display import Video, display, clear_output

from src.agents import Agent


PROGRESS_MANAGER = enlighten.get_manager()
FS_TICK: int = 12
FS_LABEL: int = 18
PLOT_DPI: int=1200
PLOT_FORMAT: str='pdf'
RC_PARAMS: dict = {
    # Set background and border settings
    'axes.facecolor': 'white',
    'axes.edgecolor': 'black',
    'axes.linewidth': 2,
    'xtick.color': 'black',
    'ytick.color': 'black',
}
ENVIRONMENT_PARAMS_PATH: str='configs/environment_params.yaml'
ENVIRONMENT_NAME: str='deepracer-v0'
MAX_DEMO_STEPS: int = 1_000
MAX_EVAL_STEPS: int = 1_000
EVAL_EPISODES: int = 5
ONLY_CPU: bool = False
SEED: int=42


def set_seed(seed: int=SEED):
    '''
    set seed for reproducability
    '''
    random.seed(seed)

    np.random.seed(seed)
    
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    
    # uncomment this for better reproducibility; slows torch
    # torch.backends.cudnn.deterministic = True
    # torch.backends.cudnn.benchmark = False
    
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    logger.info(f'Random seed set as {seed}.')


def device():
    if torch.cuda.is_available():
        device = 'cuda'
    elif torch.backends.mps.is_available():
        device = 'mps'
    else:
        device = 'cpu'
    
    if ONLY_CPU:
        device = 'cpu'
    
    logger.info(f'Using {device} device.')
    return torch.device(device)


def make_environment(
        environment_name: str=ENVIRONMENT_NAME,
        seed: int=SEED,
        **kwargs
    ):
    environment = gym.make(environment_name, **kwargs)
    
    environment = RecordEpisodeStatistics(
        FlattenObservation(environment)
    )
    
    # environment.seed(seed)
    environment.action_space.seed(seed)
    environment.observation_space.seed(seed)

    return environment


def get_world_name(
    environment_params_path: str=ENVIRONMENT_PARAMS_PATH
    ):
    with open(environment_params_path, 'r') as f:
        environment_params = yaml.safe_load(f)
    
    if 'WORLD_NAME' not in environment_params:
        raise ValueError(
            f'WORLD_NAME not defined in {environment_params_path}'
        )
    
    return environment_params['WORLD_NAME']


def get_race_type(
    environment_params_path: str=ENVIRONMENT_PARAMS_PATH
    ):
    with open(environment_params_path, 'r') as f:
        environment_params = yaml.safe_load(f)
    
    obstacles = int(environment_params['NUMBER_OF_OBSTACLES'])
    bots = int(environment_params['NUMBER_OF_BOT_CARS'])
    if (
        obstacles == 0 and bots == 0
    ):
        return 'time_trial'
    elif (
        obstacles == 6 and bots == 0
    ):
        return 'obstacle_avoidance'
    elif (
        obstacles == 0 and bots == 3
    ):
        return 'head_to_bot'
    else:
        raise ValueError(
            f'Incorrect configuration for NUMBER_OF_OBSTACLES or NUMBER_OF_BOT_CARS.'
        )


def demo(
        agent: Agent,
        environment_name: str=ENVIRONMENT_NAME,
        directory: str='./demos'               # directory to save videos
    ):
    race_type = get_race_type(
        environment_params_path=ENVIRONMENT_PARAMS_PATH
    )
    world_name = get_world_name(
        environment_params_path=ENVIRONMENT_PARAMS_PATH
    )

    demo_device = torch.device('cpu')
    agent.eval().to(demo_device)
    os.makedirs(directory, exist_ok=True)

    # create environment with proper render_mode
    demo_environment = make_environment(
        environment_name, render_mode='rgb_array'
    )

    # apply video recording wrapper
    demo_environment = RecordVideo(
        demo_environment,
        video_folder=directory,
        episode_trigger=lambda x: True,
        name_prefix=f'{world_name}-{race_type}-{agent.name}'
    )

    observation, _ = demo_environment.reset()

    demo_progress = PROGRESS_MANAGER.counter(
        total=MAX_DEMO_STEPS, desc=f'{world_name} {race_type} demo', unit='steps', leave=False
    )
    for t in range(MAX_DEMO_STEPS):
        # get action from policy
        action = agent.get_action(torch.Tensor(observation)[None, :])
        
        if not isinstance(action, np.ndarray) and torch.is_tensor(action):
            action = action.cpu().detach().numpy()
        
        if isinstance(demo_environment.action_space, spaces.Discrete):
            action = action.item()
        
        # execute the action, get observation
        observation, _, terminated, truncated, _ = demo_environment.step(
            action
        )
        demo_progress.update()
        demo_progress.refresh()
        
        if terminated or truncated:
            break
    
    demo_environment.close()
    demo_progress.close()
    
    # The RecordVideo wrapper names the file automatically with the prefix + step info
    # We'll grab the latest video with our given prefix
    # e.g. 'agent_rl-video-episode-0.mp4' or similar
    filtered_videos = sorted(
        f for f in os.listdir(directory)
        if (
            f.endswith('.mp4')
            and
            agent.name in f
            and
            world_name in f
            and
            race_type in f
        )
    )
    if len(filtered_videos) == 0:
        logger.warning('No videos found!')
        return

    # display the latest video
    video_path = os.path.join(
        directory, filtered_videos[-1]
    )

    clear_output(wait=True)
    display(
        Video(video_path, embed=True)
    )


def command_exists(command: str) -> bool:
    '''
    Check if a command exists and is executable in the system's PATH.
    '''
    return shutil.which(command) is not None


def run_command(command):    
    result=subprocess.run(
        command, capture_output=True, text=True
    )
    
    logger.info(result.stdout)
    if result.returncode:
        logger.error(result.stderr)
    else:
        logger.warning(result.stderr)


def evaluate_track(
        agent: Agent,
        world_name: str,
        environment_name: str=ENVIRONMENT_NAME,
        directory: str='./evaluations'               # directory to save eval data
    ):
    race_type = get_race_type(
        environment_params_path=ENVIRONMENT_PARAMS_PATH
    )

    logger.info(
        f'Starting {race_type} evaluation on {world_name} track.'
    )

    # restart the simulation in evaluation mode
    run_command([
        '/bin/bash',
        './scripts/restart_deepracer.sh',
        '-E', 'true',           # evaluation mode
        '-W', world_name,       # specify WORLD_NAME
    ])

    eval_device = torch.device('cpu')
    agent.eval().to(eval_device)
    os.makedirs(directory, exist_ok=True)

    # create environment with proper render_mode
    eval_environment = make_environment(
        ENVIRONMENT_NAME
    )
    observation, _ = eval_environment.reset()

    eval_metrics = {
        'progress': [],
        'lap_time': [],
    }
    evaluation_progress = PROGRESS_MANAGER.counter(
        total=EVAL_EPISODES, desc=f'Evaluating {world_name}', unit='episodes'
    )
    for episode in range(EVAL_EPISODES):
        
        episode_progress = PROGRESS_MANAGER.counter(
            total=MAX_EVAL_STEPS, desc=f'Episode {episode}', unit='steps', leave=False
        )
        for t in range(MAX_EVAL_STEPS):

            action = agent.get_action(torch.Tensor(observation)[None, :])
            
            if not isinstance(action, np.ndarray) and torch.is_tensor(action):
                action = action.cpu().detach().numpy()
            
            if isinstance(eval_environment.action_space, spaces.Discrete):
                action = action.item()

            observation, reward, terminated, truncated, info = eval_environment.step(
                action
            )

            episode_progress.update()
            episode_progress.refresh()

            done = terminated or truncated
            if done or t == MAX_EVAL_STEPS - 1:
                progress = info['reward_params']['progress']
                lap = lap_time(info)

                eval_metrics['progress'].append(
                    progress
                )
                eval_metrics['lap_time'].append(
                    lap
                )

                logger.info(
                    f'Episode {episode}:\t progress: {progress}\t lap_time: {lap}'
                )

                observation, info = eval_environment.reset()
                
                break

        episode_progress.close()
        
        evaluation_progress.update()
        evaluation_progress.refresh()
    evaluation_progress.close()
    eval_environment.close()
    
    try:
        with open(f'{directory}/{race_type}-{agent.name}.json', '+r') as f:
            all_metrics = json.load(f)
    except:
        all_metrics = {}
    
    all_metrics.update({
        world_name: eval_metrics
    })
    with open(f'{directory}/{race_type}-{agent.name}.json', '+w') as f:
        json.dump(all_metrics, f)
    
    return eval_metrics


def evaluate(
        agent: Agent,
        environment_name: str=ENVIRONMENT_NAME,
        directory: str='./evaluations'               # directory to save eval data
    ):
    race_type = get_race_type(
        environment_params_path=ENVIRONMENT_PARAMS_PATH
    )
    
    eval_world_names = ([
        'reInvent2019_wide',    # A to Z Speedway
        'reInvent2019_track',   # Smile Speedway
        'Vegas_track',          # AWS Summit Raceway
    ])
    
    eval_device = torch.device('cpu')
    agent.eval().to(eval_device)
    os.makedirs(directory, exist_ok=True)

    status = PROGRESS_MANAGER.status_bar(
        status_format=race_type + u' {fill}Evaluating {track}{fill}{elapsed}',
        color='bold_underline_bright_white_on_lightslategray',
        justify=enlighten.Justify.CENTER, track='<track>',
        autorefresh=True, min_delta=0.5
    )
    eval_metrics = {}
    for world_name in eval_world_names:
        status.update(track=world_name)
        status.refresh()
        eval_metrics[world_name] = evaluate_track(
            agent=agent,
            world_name=world_name,
            environment_name=environment_name,
            directory=directory
        )
    status.close()
    
    with open(f'{directory}/{race_type}-{agent.name}.json', '+w') as f:
        json.dump(eval_metrics, f)
    
    # restart the simulation with specified parameters
    run_command([
        '/bin/bash',
        './scripts/restart_deepracer.sh'
    ])

    return eval_metrics


def plot_metrics(
        data,
        title,
        directory: str='./plots'               # directory to save plots
    ):

    df_progress = pd.DataFrame([
        {"Track": track, "Progress": progress}
        for track, values in data.items()
        for progress in values["progress"]
    ])

    # Replace NaNs with -inf in lap time data
    df_lap_time = pd.DataFrame([
        {
            "Track": track,
            "Lap Time": lap_time if not np.isnan(lap_time) else 100_000 # default large value
        }
        for track, values in data.items()
        for lap_time in values["lap_time"]
    ])

    os.makedirs(directory, exist_ok=True)
    plt.rcParams.update(RC_PARAMS);
    sns.set_palette('deep')

    # Create the plots
    fig, ax = plt.subplots(1, 2, figsize=(8, 4))

    # Boxplot for progress
    sns.boxplot(
        x="Track",
        y="Progress",
        data=df_progress,
        ax=ax[0],
        palette='deep',
        showmeans=True,
        meanprops={
            'markerfacecolor': 'white',
            'markeredgecolor': 'black'
            },
        flierprops={'marker': 'x'}
    );

    # Boxplot for lap time (handling -inf values)
    sns.boxplot(
        x="Track",
        y="Lap Time",
        data=df_lap_time,
        ax=ax[1],
        palette='deep',
        showmeans=True,
        meanprops={
            'markerfacecolor': 'white',
            'markeredgecolor': 'black'
            },
        flierprops={'marker': 'x'}
    );
    
    fig.suptitle(title)

    # plt.xlabel(xlabel, fontsize=FS_LABEL)
    # plt.ylabel(ylabel, fontsize=FS_LABEL)
    # plt.yticks(fontsize=FS_TICK)
    # plt.xticks(fontsize=FS_TICK)
    plt.setp(ax[0].get_xticklabels(), rotation=45)
    plt.setp(ax[1].get_xticklabels(), rotation=45)
    ax[1].set_yscale('log')
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    clear_output(wait=True)

    plt.tight_layout()
    plt.show()
    fig.savefig(
        f"{directory}/{title}.{PLOT_FORMAT}", dpi=PLOT_DPI, format=PLOT_FORMAT
    )


def lap_time(info):
    if info['reward_params']['progress'] >= 100:
        if isinstance(info['episode']['t'], np.ndarray):
            # for vectorized environments
            return info['episode']['t'].mean()
        else:
            return info['episode']['t']
    else:
        # using in place of float('-inf') for better tensorboard visualizaiton
        return np.nan{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4 quick start\n",
    "\n",
    "***\n",
    "\n",
    "## Basic usage\n",
    "\n",
    "This notebook shows a basic usage of the `deepracer_gym` package along with some utility functions in `src.utils`.\n",
    "\n",
    "Make sure that you have completed the setup from the `SETUP.md` file and are using the proper python environment with this notebook.\n",
    "\n",
    "### Start the simulation service\n",
    "\n",
    "Start the simulation service container with the following command. Using this for the very first time may take some time (upto ~10 or 15 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source scripts/restart_deepracer.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also find other scripts under `scripts/` similarly useful to stop or restart the simulation service, etc.\n",
    "\n",
    "To check if the container is rumming you can use the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "apptainer instance list ||  # if using Apptainer (PACE ICE)\n",
    "docker ps                   # if using Docker (local setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that the simulator is initialized by the `agent_params.json` and `environment_params.yaml` config files in the `configs/` directory. To change the simulation settings, restart it after changing these files under the `configs/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with simulation via `deepracer_gym`\n",
    "\n",
    "We can interact with the simulation service using the familiar `gymnasium` API via the `deepracer-v0` environment provided by the `deepracer_gym` package (under `packages/`).\n",
    "\n",
    "Simply import `deepracer_gym` before using the `deepracer-v0` environment with `gymnasium` as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import deepracer_gym\n",
    "from configs.reward_function import (\n",
    "    # provide your own custom reward function if needed.\n",
    "    # if not provided, gymnasium environment fetches it from configs.reward_function\n",
    "    reward_function\n",
    ")\n",
    "\n",
    "env = gym.make(\n",
    "    'deepracer-v0',\n",
    "    reward_function = reward_function\n",
    ")\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "observation, reward, terminated, truncated, info = env.step(\n",
    "    env.action_space.sample()\n",
    ")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the output dimensions of the observations\n",
    "{\n",
    "    k: v.shape for k, v in observation.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Utility functions and features\n",
    "\n",
    "### Flattened environment\n",
    "\n",
    "Notice that the `observation` variable above is a dictionary (keys are sensor names, values are measurements). Such a data-structure is a bit more difficult to handle than simple vectors, especially for batching purposes. Therefore, we suggest that you use the provided `src.utils.make_environmrnt` function instead. It 'flattens' the observation space into a single vector space using the `gymnasium.wrappers.FlattenObservation` class.\n",
    "\n",
    "Additionally, it also wraps the environment with a `gymnasium.wrappers.RecordEpisodeStatistics` class, which can be very convenient for calculating things like episode langth and returns.\n",
    "\n",
    "Please get familiar with all of these functions/ classes before attempting the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import make_environment\n",
    "\n",
    "env = make_environment(         # just replace gym.make\n",
    "    'deepracer-v0'\n",
    ")\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "observation, reward, terminated, truncated, info = env.step(\n",
    "    env.action_space.sample()\n",
    ")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the output dimensions of the observations\n",
    "observation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Observation encoder\n",
    "\n",
    "We can un-flatten, pre-process and encode the flattened observations above using an **example/default** encoder implementation provided as `src.transforms.EncodeObservation`. An example usage is given below, feel free to change/ modify this implementation and its components<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) according to your needs.\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) Make sure to appropriately pre-process your observations under `src.transforms.PreprocessLiDAR` and `src.transforms.PreprocessCamera`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.run import tensor\n",
    "from src.utils import device\n",
    "\n",
    "from src.transforms import EncodeObservation\n",
    "\n",
    "DEVICE = device()\n",
    "# encode the observations\n",
    "encoder = EncodeObservation().to(DEVICE)\n",
    "encoded_observation = encoder(\n",
    "    tensor(observation)\n",
    ")\n",
    "\n",
    "# see the output dimensions of the encoded observations\n",
    "encoded_observation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize agent policy\n",
    "\n",
    "So long as your agent implements a `get_action` method as in `src.agents.py`, you can use our provided `src.utils.demo` function to visualize the policy of the agent in the form of a MP4 video saved under `demos/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import demo\n",
    "from src.agents import RandomAgent\n",
    "\n",
    "agent = RandomAgent(environment=env)\n",
    "demo(agent.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on multiple tracks\n",
    "\n",
    "You can use the provided `src.utils.evaluate` function to evaluate your agent on all three project tracks (for any specified race type in `configs/environment_configs.yaml`). The results are both returned as well as saved (and over-written) under `evaluations/`.\n",
    "\n",
    "Please note however that this funciton basically re-starts the simulation a number of times to switch between the tracks for evaluation and this may be a bit time-consuming (~5 minutes on a PACE ICE machine for an untrained agent, and ~20 minutes for a fully trained agent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import evaluate\n",
    "\n",
    "metrics = evaluate(\n",
    "    agent.eval()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting evaluation metrics\n",
    "\n",
    "You can plot the returned evaluation metrics dictionary using the provided `src.utils.plot_metrics` functions. The results are also saved under `plots/`.\n",
    "\n",
    "Note however that you do not necessarily have to stick to this exact visualization/plot, and may make adjustments as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import plot_metrics\n",
    "\n",
    "plot_metrics(\n",
    "    metrics, title='usage'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Training and logging\n",
    "\n",
    "Please use the structure in `src.run.py` to design your training and logging loops. Importantly, try to use `tensorboard` if you can, for example as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.run import run\n",
    "\n",
    "my_hyper_parameters = {\n",
    "    'lr': 4e-4\n",
    "}\n",
    "run(my_hyper_parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the training logs, use `tensorboard` by running the following command in your terminal.\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir runs\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepracer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
